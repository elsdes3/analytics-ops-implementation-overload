{
  
    
        "post0": {
            "title": "Automate Creation and Use of a Structured Data Science Portfolio Project",
            "content": "Automate Creation and Use of a Structured Data Science Portfolio Project . Background and Motivation . The Use-Case . Starting a new data analytics project can be stressful, repetitive or annoying. Actually, it can be all of these combined. We are thinking about the business use-case (user, client, etc.) for the project we’re about to begin working on. We want it to be our implementation of a real-world use-case and we’re trying to correctly identify the business user (client) and their requirements. Additionally, we’re dealing with constraints imposed by data, such as when the data will be available to use, if there is enough of it and what supplementary datasets are needed. . The Problem . If we’ve got a previous project structure, it becomes tempting to copy-paste that structure into a new folder and begin changing files. This becomes annoying because we want to focus on the use-case for the project, and the related issues mentioned above, and not the drudgery of 56 mouse-clicks to set up one project. We know that this process has to be repeated for the next project. This process is susceptible to errors, missing files and missing content. . Build on Existing Proven Solutions . The recommended approach to creation of a new data science project is to spend the time once to build a project template that we can use across multiple projects and automate this project set-up process. The cookiecutter-data-science project template does precisely this. It can be used to effortlessly and reliably start up a new project. . The Need for Customizing and Extending An Available Solution . But what happens if we want to go beyond this and enhance the functionality of such a template. For example, if we want to . create a project directory structure, pre-populate analysis (sample) code to use this structure and then run this code to see the output | specify logical code-formatting requirements and run checks on any pre-populated code in our project | set up all the Python virtual environments we need | set up a continuous-integration workflow to verify that our end-to-end workflow (including code-formatting checks) runs after we have made any changes to our code base | ensure our analytical tools (Python libraries) stay up to date | . | . then we need to expand on the cookiecutter-data-science template. That project has provided us with a solid base, but it is up to us to customize it to our preference. This is why the cookiecutter-portfolio template (link) was created. . A Candidate Solution . cookiecutter-portfolio goes a little further than cookiecutter-data-science. It implements the four enhancements discussed above (and a few others too), and makes opinionated modifications to the solid base provided by cookiecutter-data-science in order to accomplish this. Its underlying goals are to . make several decisions for the user in preparing minimal pre-populated content in files two sample notebooks for data analysis | baseline implementation of code-formatting checks | . | configure programmatic execution of analysis during CI using Github Actions | run the probided minimal (sample) analysis code from start to finish, and perform code-formatting checks | . This template can be used to automate the set-up and continuous development of a Python-based data analytics project that a user wants to host on Github. . About this blog post . This blog post is intended to explain the following about the cookiecutter-portfolio template . how to use it to create a new data analytics project | what are the files and folders produced by the template, and what is their purpose | how does it differ from cookiecutter-data-science | . Using the cookiecutter-portfolio template . Step-by-Step Instructions to create a project using cookiecutter-portfolio . Install the git system package from here. . Next, install the necessary Python packages . tox (link) this will be used to manage virtual environments | . | cookiecutter (link) this is required since the cookiecutter-portfolio template was created using the cookiecutter Python library | . | . using . pip3 install tox cookiecutter . A templated portfolio project can be created from this template by cloning the source code from github . git clone https://github.com/elsdes3/cookiecutter-portfolio . Next, change into the root directory of the project . cd cookiecuter-portfolio . and run the make target build using . make build . These instructions have only been verified on Linux and MacOS systems. Currently, verification is not available on Windows-based systems. . Walkthrough of the portfolio project that is created . Document our analysis in Jupyter notebooks . Initial development of data science analysis is commonly done using Jupyter notebooks, which are documents consisting of code (including multiple programming languages), rich text, figures, images, links, etc. In this template, notebooks are named using a numbering system. A two-digit number is specified in the prefix of the notebook filename. Notebooks should be run in chronological order of their filename. One approach to using multiple notebooks is to limit each notebook to performing a single step in our overall workflow. . A logical sequence of workflow steps and their corresponding notebooks might be the following . retrieve raw data 01_get_raw_data.ipynb | . | process raw data 02_process_data.ipynb | . | explore processed data 03_explore_processed_data.ipynb | . | analyze processed data 04_analyze_processed_data.ipynb | . | . This template provides sample code for the first two of these steps (notebooks, 1 and 2). . Programmatically execute our notebooks . How can we be sure that our code runs from start to finish? When we’re sharing code in our portfolio project, we want to be sure that our analysis is free of errors. In a data science portfolio project, this amounts to having several notebooks that run from start to finish. If each notebook captures a single step in our analysis workflow, then we would want to run all notebooks in succession and verify that we don’t get any errors. If we define notebook inputs using papermill_runner.py, then this file will allow us to programmatically execute a notebook using Python. The benefit of this is that we can automate the execution of an entire workflow (consisting of several notebooks). If we’ve taken care to ensure our analysis does indeed run as we expect it to, from start to finish, then this should make us confident in having others also run our code from themselves and follow the logic we have implemented in our code. Using papermill is a convenient approach to be sure that our portfolio project contains Python code that is free of errors. . Papermill requires a single cell with Python variables that can be programmatically changed. This cell should be marked with a parameters tag. See these instructions for adding a tag to a cell. In the sample notebook 01_get_data.ipynb, these parameters are . num_data_files = 5 num_rows = 10 column_names = [&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;, &quot;E&quot;] . and in 02_process_data.ipynb, these parameters are . processed_data_file_name = &quot;processed&quot; . The papermill_runner.py script contains these python parameters in dictionaries for the two sample notebooks provided with this template (one_dict and two_dict for 01_get_data.ipynb and 02_process_data.ipynb respectively). In this script, these two notebook names are assigned to the variables one_dict_nb_name and two_dict_nb_name respectively. . To add more notebooks (eg. 03_explore_processed_data.ipynb) to this template, the following changes must be made . add a new dictionary of input variables (eg. three_dict) to nb_dict_list | add a new notebook name (eg. three_dict_nb_name) to nb_name_list | . Make it easier for others to run and use our analysis . Can other users run our data analysis online? It is convenient for others to run our code if they don’t need to download the project locally. It shows we care about visitors looking to explore our portfolio projects. We can facilitate this using the MyBinder service. A visitor to our project on Github would need to specify the environment using environment.yml. So, a sample version of this file is provided by this template. The visitor can then follow instructions in the basic example from the MyBinder documentation to create an online computing environment to run the notebooks in our portfolio project. . Correctly handle the use of Python virtual environments . This is a template based on Python. It is best for Python code to run in a self-contained and independent environment from the host system. One way to achieve this is through the use of a Python virtual environment. We will use the tox Python library to manage all our Python virtual environments. These are specified in tox.ini, which will act as the central hub for installing all Python dependencies required for us to run the Python code that preforms the data analysis in our propject. This file will install the Python packages specified in requirements.txt. . Keep sensitive info and files away from version control systems (git) . For obvious reasons, our portfolio should not contain information like passwords. It makes us seem uninterested in paying attention to details and unreliable with sensitive data. cookiecutter-portfolio assumes that git (link) will be used for version control (and that we’ll be posting our portfolio projects to GitHub). To ensure we don’t commit files containing sensitive or unnecessary information to git, a .gitignore file is provided. The contents of this file are specific to Python and are taken from gitignore.io. We should add files that we don’t want git to track to this .gitignore file. . Ensure our code is properly formatted before it is uploaded to Github . When we’re publishing code for a portfolio project, we want the code to be properly formatted. As an example, we don’t want our custom Python module to contain lines of code that are 350 characters long. There are many other best practices. See PEP8 and the Google Style Guide for Python for inspiration. This is especially important for collaborative projects. Code formatting is enforced using .pre-commit-config.yaml. This file will prevent code from being pushed to git if it does not meet requirements such as newline file-endings (1, 2). . Co-ordinate execution of our workflows using a Makefile . Running notebooks programmatically, manually running our code formatting checks and starting the jupyter notebook server for local development are initiated by their respective CLI commands. We don’t want to remember this every time we need to update our project, push changes to github, etc. For example, if we make a change to the second notebook in our project then we should be able to quickly run all the notebooks in the project to ensure that the chage we just made has not caused errors elsewhere in the analysis. Papermill is a quick way to do this but we need to remember the command to use this tool. Over time, we will get better at remembering these commands. But what about visitors to our portfolio project on Github? Should we also expect that they too will know the exact commands that are needed to perform these tasks in our project? A better approach is to simplify this as much as possible by reducing the amount of command-line code to be called in order to perform a task. We will do this with make (link). On most Linux distributions, make comes pre-installed. Windows users can download cmake from here and follow this video to install it. . To minimize user input, a Makefile (link) is provided which runs the relevant command in each Python environment. The following make targets are provided . . build …starts a Jupyter notebook server . . precommit …manually runs all code-formatting checks defined in .pre-commit-config.yaml . . ci …programmatically runs all notebooks in the project . . clean …cleans up Python artifacts . Extract and Outsource code into Python modules . We’ll now focus on the folders that are created, excluding the folder with code for our custom Python package. Different types of files will be saved in these folders. Below is a quick overview of each folder . data/raw raw data that is retrieved in 01_get_data.ipynb will be stored in the raw/ sub-folder within data/ | . | data/processed data that is processed using 02_process_data.ipynb will be stored in the processed/ sub-folder within data/ | . | models any trained machine learning models should be serialized (scikit-learn, keras, spaCy) to this location on the disk | . | executed_notebooks when papermill executes a jupyter notebook, it saves a copy of the executed notebook in this folder | . | references here we can store whitepapers, research papers, etc. from our background research about the topic of our analysis | . | reports/figures if we choose to save figures we generated in the Jupyter notebooks to disk, then we should export them to the figures/ sub-folder within reports/ | . | . Finally, we’ll discuss the contents of the Python package that is created. The default name for this package is specified in config.yaml to be {{cookiecutter.package_name}}. For more discussion about package naming and structure see the following links . The Hitchhiker’s Guide to Python | Github issue in a repository of the Python Packaging Authority | Documentation by Python Packaging Authority | Blog Post on the Medium publishing platform | Question on Stackoverflow | . Inside this package are . four Python sub-packages (1, 2, 3) | one blank file __init__.py file | one Python module utils.py | . __init__.py makes Python recognize the contents of the package ({{cookiecutter.package_name}} folder) as Python modules. This means we can import modules in the sub-packages into a Python script or Jupyter notebook (with a Python kernel) using, as an example, the following approach to use the visualization sub-package . from {{cookiecutter.package_name}}.visualization import visualize as vis . This is similar to how we would have imported a module from the numpy Python package . from numpy import sum as np_sum . One of the main benefits of extracting common code from our Jupyter notebook and placing it in such modules is to keep the Jupyter notebooks as focused as possible without distracting the reader. Read more about the benefits of refactoring code here. The cookiecutter-data-science template provides blank modules in each sub-package. . In terms of re-factoring code into modules, the cookiecutter-portfolio template . adds two sample modules to the data sub-package to demonstrate basic examples of retrieving ({{cookiecutter.package_name}}/data/load_data.py) raw data | processing ({{cookiecutter.package_name}}/data/process_data.py) raw data | . | adds one sample module in {{cookiecutter.package_name}}/utils.py with a few custom utilities for exploring data stored in DataFrames (link) | shows examples of best-practices at developing custom modules using docstrings for all methods in the sample modules | Python type-annotations for all parameters in methods in the sample modules | . both of which improve the readability of the content in these modules . | . Below is a brief overview of the four Python sub-packages and the Python module in {{cookiecutter.package_name}} . {{cookiecutter.package_name}}/data contains three modules to retrieve and process raw data | . | (optional) {{cookiecutter.package_name}}/features contains a module to build features for the machine learning (ML) step of a project (if present) | . | (optional) {{cookiecutter.package_name}}/models contains two modules to train a ML model and make predictions with the trained model, in the ML step of a project (if present) | . | {{cookiecutter.package_name}}/visualization contains a module to visualize processed data | . | {{cookiecutter.package_name}}/utils.py a Python module containing convenience utilities for exploring a DataFrame | . | . Licensing . Can users use our analysis in their own portfolios? If our analysis is useful, this would be a great way to get some recognition for our hard work. An open-source license is provided to allow visitors to understand how they can use the code in our portfolio project and whether we need them to give us credit. To get started with using open source licenses for data science projects, see this blog post. . Customize the cookiecutter-portfolio template . The cookiecutter-portfolio template was developed with the intention of being used as-is with no need for changes. A project will be created and the two sample notebooks will be programmatically executed allowing us to immediately start developing code in 01_get_data.ipynb. . If we would like to make changes to the cookiecutter-portfolio template, then the above walkthrough has provided us with a baseline understanding of the folder structure of the project that the template produces for us. This knowledge will help us to further personalize (modify) this template to suit our own custom workflows. Refer to the documentation for the cookiecutter Python package for help with the basics of templating using cookiecutter. In particular, the sections Getting to Know Cookiecutter and Create a Cookiecutter From Scratch are useful starting points. . If we make any changes, then we can quickly check that the template is still rendered correctly, code formatting checks still pass and that the execution of the sample notebooks (in the template folder) runs through to completion by simply running the following from the root folder (like we saw in the Usage section above) . make build . That’s all. If the changes we made to the template were correctly implemented then everything should run through without any errors. . Differences between cookiecutter-portfolio and cookiecutter-data-science . This section of the blog post is only needed to compare cookiecutter-portfolio to the Python cookiecutter it is based on (cookiecutter-data-science) and understand the differences. This section does not affect how to use the cookiecutter-portfolio template. . As was mentioned earlier, cookiecutter-portfolio is based on the cookiecutter-data-science template. We will now discuss how cookiecutter-portfolio is different from cookiecutter-data-science. This discussion will help to clarify the need for creating this template instead of using cookiecutter-data-science. . Changes in the created project template relative to cookiecutter-data-science . We will first discuss changes made to the template itself. This means any changes inside the {{cookiecutter.repo_name}} folder. . We will start by listing the files and folders from the cookiecutter-data-science template that were deleted . The notebooks folder is deleted. The notebooks will be placed directly in the root of the project. | docs is deleted. This feature was aimed at adding documentation for the {{ cookiecutter.module }} Python module. In our case, for simplicity, we will not offer this feature with the template but it can be easily brought back later if required. | .env is removed from the root of the project. This file is used to store credentials that will be injected as environment variables using the python-dotenv library, which will iteratively walk up through the file system, starting from this project’s root directory, until it finds this file. So, the file can be located one level above this project’s root. The benefit of this is that we can re-use the same credentials for multiple projects. | We’ll now discuss files that have been changed . .gitignore is modified such that data/ and its sub-folders (excluding contents) are not ignored by git. This means, data/ and its (empty) sub-folders will be kept on github and won’t have to be manually created after we pull code from the repository. | tox.ini is modified to manage all Python virtual environments required for this project. See a basic tox.ini configuration file here. See the tox documentation for more details. | {{ cookiecutter.module_name }} renamed to {{ cookiecutter.package_name }} to be in line with standard Python conventions about the differences between packages and modules (1, 2) | renamed to {{cookiecutter.package_name}} since .pre-commit-config.yaml could not correctly detect the leading and trailing space in the name of this folder | data two sample Python sub-modules load_data.py and process_data.py 1 are added | . | utils.py a few simple Python methods are included in this module to support brief exploration of DataFrames | . | . | More details have been added to the provided README.md file. | A set of commonly-used Python libraries have been added to requirements.txt 1. | The following files have been added . environment.yml is provided to support launching or sharing the project on the MyBinder service. | papermill_runner.py is added to support executing notebooks. This is done using the papermill Python API. See the papermill documentation for more details. | .pre-commit-config.yaml is added to ensure files and code follow proper formatting (eg. code linting) before every commit to git. See the pre-commit Quick start to get started using this framework. | The executed_notebooks folder is added as the location where notebooks executed with papermill will be saved. | Two sample notebooks to replicate downloading (01_get_data.ipynb) and processing (02_process_data.ipynb) raw data 1. | The following files and folders are not been changed (except for sorting imports in files) . setup.py | data and its sub-folders | models | references | reports | {{cookiecutter.package_name}} data/make_dataset.py | features | models | visualization | . | Besides differences in the directory structure and file contents, cookiecutter-portfolio provides and runs two sample notebooks that are provided. By comparison, cookiecutter-data-science doesn’t offer or run notebooks after rendering the template. Offering a sample notebook makes it a little easier for a user to start developing code using the created Python modules in {{cookiecutter.package_name}} and the templates directories (eg. data/raw, data/processed, executed_notebooks) after generating the template. The user can quickly adapt this code for use in future steps in the workflow (i.e. in other notebooks). . Changes at the project level relative to cookiecutter-data-science . Next, at the repository level, the implementation of cookiecutter-portfolio is fundamentally different to that in the cookiecutter-data-science template. Specifically, the implementation of this project (cookiecutter-portfolio) is purely that of a template that only follows the structure of a Python cookiecutter. By comparison, the cookiecutter-data-science template can be installed in editable mode - this is not supported by cookiecutter-portfolio. . tox is used for managing python virtual environments at the repository level so it is added to requirements.txt. The other requirements file, requirements-test.txt, is only used to specify Python packages to use in the unit testing virtual environment. Tests are implemented exclusively using pytest-cookies, a framework designed specifically to test generation of templates created using cookiecutter. . As such, two significant changes have been made . tests, {{cookiecutter.repo_name}} and hooks are placed inside a cookiecutter-project sub-folder. This is done so that the user can generate this (cookiecutter-portfolio) template without any command-line prompts. To do this, the documentation from the cookiecutter project was used to providing the command line arguments required to suppress command-line prompts --no-input | --config-file | . | specify a user configuration file (config.yaml) by providing this file, the user can directly modify it following the instructions in the README.md file (see Usage and Notes) rather than needing to manually create these files and populate their contents appropriately. | . | . | Several files in the template are not required in cookiecutter-portfolio ccds 1 | ccds.json 2 | dev-requirements.txt a standard development environment is not supported here | . | netlify.toml Netlify is not supported here | . | setup.py this template does not support installation in editable mode so use of this file is not supported here | . | . | Summary . With the cookiecutter-portfolio template, we have access to an project built on the strong foundations of cookiecutter-data-science with . programmatic execution of data analysis | pre-populated code-formatting framework | virtual environments to manage all Python code | an implementation of continuous integration to ensure any modifications to our code-base result in analysis that still executes from start to finish | code whose formatting follows best practices | . | . Combined, these add a layer of automation to the strong base offered to us by cookiecutter-data-science. . Contributing to cookiecutter-portfolio . Contributions to cookiecutter-portfolio are very welcome, and are greatly appreciated. Every little bit helps, and credit will always be given. . If you do decide to use the cookiecutter-portfolio template and have recommendations to improve the structure of this template or corrections to mistakes, create an issue on Github. . Tests can be run with . make test clean-tests . Please make sure that all tests are green before submitting a pull request on Github. . These are only provided as a starting point for loading and processing data respectively. These are not repersentative of any real-world workflow. &#8617; &#8617;2 &#8617;3 &#8617;4 . | It is not indended that cookiecutter-portfolio should be called using the command ccds. &#8617; . |",
            "url": "https://elsdes3.github.io/analytics-ops-implementation-overload/markdown/cookiecutter/data-science/portfolio/template/python/2022/01/24/project-template-post.html",
            "relUrl": "/markdown/cookiecutter/data-science/portfolio/template/python/2022/01/24/project-template-post.html",
            "date": " • Jan 24, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Implementing Machine Learning Ensembles of Decision Trees in Python",
            "content": "Background . In this post, we will implement two popular tree-based machine learning (ML) algorithms from scratch. Usually, such approaches can improve the accuracy of predictions made with a single ML model by combining these predictions. There are several flavours of ensembling techniques that are used in ML. Here, we will focus on two of them - bagging (or bootstrapping) and boosting). . Goal . Here, we will use Python to implement these two ensemble algorithms from scratch - random forest for ML (binary) classification and gradient boosting for ML regression tasks. Both ensembling techniques use tree-based algorithms. We&#39;ll use each implemented ensemble with a synthetic dataset. They will be trained and used to make predictions that will be scored on data that was not used during training. These scores will be compared to the corresponding scores from each algorithm&#39;s implementations provided by the scikit-learn library in Python. If our implementation has correctly captured the ensembling logic, then our manual approaches should agree with the versions provided by scikit-learn. . The focus here will on implementing the ensembles. To stay within scope, we&#39;ll use the individual decision tree ML models provided by scikit-learn and won&#39;t implement a single decision tree from scratch. . Overview of Random Forest . The main principle behind the random forest algorithm is that multiple decision trees will be trained on separate (random) subsets of the training data. Each of these tree is then used to make a prediction. For classification, the most commonly occurring of these predictions will be chosen. . An overview of the working of this algorithm is as follows . Draw a bootstrap sample of the training data | Train a single decision tree on this random sample | Use the trained decision tree to make a prediction on unseen (test) data | Repeat the above three steps for as many individual decision trees that we want. | Among all the predicted classes made by all decision trees in step 4., choose the most commonly occurring ones and return these as the final prediction for the observations in the unseen data. | Manual Implementation of Random Forest . We&#39;ll begin the implementatoin of this algorithm by creating a class with the following parameters that will be passed to the individual decision trees that we will train . min_samples_split the minimum samples required for a split | . | max_depth | num_features | . This is not the full set of hyper-parameters that can be passed to a single decision tree. More can be added later if required. . All these parameters will be passed to the __init__ method of this class. We are also inheriting from the BaseEstimator and RegressorMixin classes so that this class is compatible with the scikit-learn API. . As mentioned earlier, a list of decision trees will be built during training and be called upon to make predictions. We&#39;ll instantiate this list in the __init__ of this class so it can be used in the other methods of this class. . class CustomRandomForestClassifier(BaseEstimator, ClassifierMixin): def __init__(self, min_samples_split=5, max_depth=25, num_features=None, num_trees=50): self.min_samples_split = min_samples_split self.max_depth = max_depth self.num_features = num_features self.num_trees = num_trees self.trees = [] . The fit method will start by iterating over our specified range of trees specified as the num_trees parameter. We&#39;ll pass the parameters defined in the __init__ method that are required by a single decision tree. Here, these will be min_samples_split, max_depth and num_features. . Then, we will train this tree on a random bootstrap sample of the training data. We&#39;ll extract this sample using a helper function draw_bootstrap_sample(). This function will randomly select from zero to all the available training observations. We will draw samples with replacement, meaning we will randomly drop some training observations and replace them with duplicates of the other (randomly selected) observations. . Each decision tree will be trained on this sample of the data passed to it and the trained tree will be appended to the empty list we instantiated earlier in __init__. . def draw_bootstrap_sample(X, y): num_samples = len(X) random_indexes = np.random.choice(num_samples, num_samples, replace=True) X_sample, y_sample = X[random_indexes], y[random_indexes] return X_sample, y_sample @staticmethod def train_bootstrap_sample(min_samples_split, max_depth, num_features): clf = DecisionTree( min_samples_split=min_samples_split, max_depth=max_depth, n_feats=num_features, ) X_sampled, y_sampled = draw_bootstrap_sample(X, y) clf.fit(X_sampled, y_sampled) return clf def fit(self, X, y): self.trees = [ train_bootstrap_sample( self.min_samples_split, self.max_depth, self.num_features, ) for _ in range(self.num_trees) ] . where train_bootstrap_sample() is a static method (1, 2) for our CustomRandomForestClassifier class. . The predictions will iterate over the list of trained decision trees and use each of them to make predictions. We&#39;ll store these predictions (slices of the training data) in an array. . Each element of this array is the prediction of a slice of the training data. We&#39;ll reshape this array such that all predictions of the first sample are immediately ahead of all predictions of the second sample which in-turn are immediately ahead of all predictions of the third sample, and so on. This way, we can extract the most common prediction of the each sample by iterating over this reshaped array and picking the most commonly occurring element (prediction) during each iteration. The numpy library&#39;s .swapaxes() method does this for us. There are other ways to extract the most commonly occurring prediction from the array of predictions, so this approach is only for convenience. . Finally, we&#39;ll append all of the most common predictions to a separate blank list, which we&#39;ll convert to an array and return as the final predictions of the ensemble algorithm. . def predict(self, X): # use each trained decition tree to make a prediction y_preds = np.array([clf.predict(X) for clf in self.trees]) # reshaping (for convenience) y_preds = np.swapaxes(y_preds, 0, 1) # return the most commonly predicted class label across # predictions by all trees y_pred_list = [] for y_pred in y_preds: if not self.threshold: # without using probabilities y_pred_single = Counter(y_pred).most_common(1)[0][0] else: # using probabilities y_pred_single = use_probabilities_to_get_labels(y_pred, 0.5) y_pred_list.append(y_pred_single) y_pred = np.array(y_pred_list) return y_pred . The final complete class to realize this implementation is shown below . def draw_bootstrap_sample(X_full, y_full): num_samples = len(X_full) random_indexes = np.random.choice(num_samples, num_samples, replace=True) X_sample, y_sample = X_full[random_indexes], y_full[random_indexes] return X_sample, y_sample def use_probabilities_to_get_labels(y_train_labels, threshold=0.5): # get average of training labels y_pred_proba = np.mean(y_train_labels) # assign test labels by comparing average label value to threshold y_pred = 0 if y_pred_proba &lt; threshold else 1 return y_pred . class CustomRandomForestClassifier(BaseEstimator, ClassifierMixin): def __init__( self, min_samples_split=5, max_depth=25, num_features=None, num_trees=50, threshold=None, ): self.min_samples_split = min_samples_split self.max_depth = max_depth self.num_features = num_features self.num_trees = num_trees self.threshold = threshold self.trained_decision_trees = [] @staticmethod def train_bootstrap_sample(X, y, min_samples_split, max_depth, num_features): tree_ = DecisionTreeClassifier( min_samples_split=min_samples_split, max_depth=max_depth, max_features=num_features, ) # Draw bootstrap sample X_sampled, y_sampled = draw_bootstrap_sample(X, y) # Train tree_.fit(X_sampled, y_sampled) return tree_ def fit(self, X, y): # Train multiple decision trees on separate bootstrap # samples drawn from the training data self.trained_decision_trees = [ self.train_bootstrap_sample( X, y, self.min_samples_split, self.max_depth, self.num_features, ) for _ in range(self.num_trees) ] @staticmethod def make_single_prediction(test_obs_indx, y_pred_reshaped, threshold=0.5): if not threshold: # without using probabilities (binary or multi-class) y_pred_single = Counter(y_pred_reshaped).most_common(1)[0][0] else: # using probabilities (binary only) y_pred_single = use_probabilities_to_get_labels(y_pred_reshaped, threshold) return [test_obs_indx, y_pred_single] def predict(self, X): # Make predictions with each trained decision tree executor = Parallel(n_jobs=cpu_count(), backend=&#39;multiprocessing&#39;) tasks = ( delayed(tree_.predict)(X) for tree_ in self.trained_decision_trees ) y_preds = np.array(executor(tasks)) # reshaping (for convenience) y_preds = np.swapaxes(y_preds, 0, 1) # return the most commonly predicted class label across # predictions by all individually trained decision trees tasks = ( delayed(self.make_single_prediction)(k, y_pred, self.threshold) for k, y_pred in enumerate(y_preds) ) y_pred = np.array(executor(tasks)) # sort by index (first column) to ensure same index as X, and # get extract second column (labels) y_pred = y_pred[np.argsort(y_pred[:, 0])][:, 1] return y_pred . where use_probabilities_to_get_labels() is a Python function to convert prediction probabilities into hard labels using a user-adjustable discrimination threshold of 0.5. . Since we can make predictions in any order, we can parallelize the calls to the .predict() method of each trained decision tree to do this. Similarly picking the most commonly occuring predicted label can be done in parallel for each tree&#39;s predictions of the same sample, as long as we keep track of the index of the test data and sort by this when re-assembling the most commonly occuring value. The benefits of these changes will be seen if a larger number of trees is specified in this ensemble ML model. These changes have been made above in order to improve the speed of the .predict() method. . Using the custom implementation of Random Forest for an ML classification task . We&#39;re now ready to use this custom implementation to predict observations from a dataset. We&#39;ll use a synthetic dataset for this purpose and we&#39;ll evaluate the performance of the custom implementation by scoring its predictions against those made by the built-in version of this algorithm provided by scikit-learn. . We&#39;ll now generate the synthetic classification of 20,000 rows and 15 features, and we&#39;ll specify that 10 of these features should be useful for predicting the class labels . nrows = 15_000 n_feats = 10 n_informative_feats = 9 flip_y = 0.4 # Gradient Boosting data properties n_samples_gb = 20_000 n_feats_gb = 25 n_informative_feats_gb = 15 noise_gb = 25 . X, y = make_classification( n_samples=nrows, n_features=n_feats, n_informative=n_informative_feats, n_redundant=0, n_classes=2, flip_y=flip_y, # values closer to 1.0 increase difficulty of classification n_clusters_per_class=1, class_sep=1.0, random_state=0, ) df_X = pd.DataFrame(X, columns=[f&quot;var_{f_i}&quot; for f_i in range(1, n_feats+1)]) s_y = pd.Series(y, name=&quot;target&quot;) . The first 5 rows of the generated dataset are shown below . First ten rows of independent (var_*) and dependent (target) variables in the data &nbsp; var_1 var_2 var_3 var_4 var_5 var_6 var_7 var_8 var_9 var_10 target . 0 -1.351186 | -2.093260 | 0.975290 | 2.313769 | -0.542349 | 3.762062 | -2.942417 | -0.498531 | -0.675987 | 2.149942 | 0 | . 1 -2.367559 | 0.211892 | 2.422790 | 0.134927 | 0.249414 | -3.653938 | -0.815030 | 1.223067 | 0.933831 | -0.505265 | 0 | . 2 0.038928 | 0.268696 | -0.502049 | 0.204705 | -0.250988 | 0.290400 | -2.111975 | 0.962967 | -1.283997 | 2.250411 | 1 | . 3 -1.889555 | -0.034488 | -0.001209 | 3.668849 | -2.930176 | 1.353577 | 0.505302 | -0.614030 | -1.797459 | -1.235999 | 1 | . 4 -1.376305 | -2.345263 | -1.106297 | 3.705584 | 2.350064 | 0.329091 | 1.900438 | 1.232044 | -1.652215 | -2.146132 | 0 | . Splitting the data . We&#39;ll now divide the data into training and testing splits . X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30) . The classes in the training split from the synthetic dataset are balanced, as shown below . So, we don&#39;t need to re-balance the data before passing it to the Random Forest algorithm. . Pre-Processing . We will assume that we are looking to interpret the importance of features used by these implementations. Note that our manually implemented version does not support this at the moment. To this end, we need to remove correlated features from the data. . Tree-based models are not sensitive to the scale of the features, but can be incorrect to interpret their contribution to the predictions if they are not all on the same scale (see Strobel et al 2007). So, the second pre-processing step we need is to standardize our data before it is used to train the Random Forest algorithm. . A heatmap of the feature-to-feature correlations is shown below . We&#39;ll remove correlated features based on this heatmap . cols_to_drop = [&quot;var_1&quot;, &quot;var_2&quot;, &quot;var_6&quot;, &quot;var_7&quot;] selected_cols = list(set(list(df_X)) - set(cols_to_drop)) X_train = pd.DataFrame(X_train, columns=list(df_X)).drop(columns=cols_to_drop).to_numpy() X_test = pd.DataFrame(X_test, columns=list(df_X)).drop(columns=cols_to_drop).to_numpy() . Comparing the Manual and Built-in Implementations of Random Forest . We&#39;ll define a pipeline to normalize the data before passing it to the manual implementation of the random forest algorithm . %%time pipe_manual = Pipeline( [ (&quot;ss&quot;, MinMaxScaler()), (&quot;reg&quot;, CustomRandomForestClassifier(5, 25, None, 100, 0.5)), ] ) pipe_manual.fit(X_train, y_train) y_pred_manual = pipe_manual.predict(X_test) . CPU times: user 6.22 s, sys: 133 ms, total: 6.35 s Wall time: 6.45 s . We&#39;ll do the same with the built-in version of the algorithm . %%time pipe_builtin = Pipeline( [ (&quot;ss&quot;, MinMaxScaler()), ( &quot;reg&quot;, RandomForestClassifier( min_samples_split=5, max_depth=25, max_features=None, n_estimators=100, ) ), ] ) pipe_builtin.fit(X_train, y_train) y_pred_builtin = pipe_builtin.predict(X_test) . CPU times: user 4.43 s, sys: 0 ns, total: 4.43 s Wall time: 4.43 s . Below are the first and last five rows of the testing data, along with the true and predicted labels found using the manually implemented and built-in Random Forest algorithms . First and Last 5 predictions of test split &nbsp; var_9 var_10 var_3 var_5 var_4 var_8 true manually_implemented_knn built_in_knn . 0 -0.32 | 1.47 | 0.45 | 2.12 | -0.66 | 2.20 | 1 | 1 | 1 | . 1 -0.10 | 1.15 | 2.16 | 1.42 | 0.12 | 0.01 | 0 | 0 | 0 | . 2 -1.38 | 0.25 | 3.34 | -1.41 | -2.87 | 0.81 | 0 | 0 | 0 | . 3 0.26 | 2.37 | -0.70 | -0.16 | -0.70 | 2.61 | 1 | 1 | 1 | . 4 -0.78 | -0.66 | -1.28 | 2.00 | 1.75 | 1.33 | 0 | 1 | 1 | . 4495 0.28 | 1.25 | 0.90 | 0.14 | 2.00 | -0.29 | 0 | 0 | 0 | . 4496 -0.42 | 2.40 | 0.48 | 1.93 | -1.59 | -1.76 | 1 | 1 | 1 | . 4497 0.17 | 3.98 | -1.02 | -0.32 | -4.51 | -0.65 | 1 | 1 | 1 | . 4498 0.19 | 2.54 | -0.26 | -0.70 | 1.69 | 0.03 | 0 | 0 | 0 | . 4499 -1.71 | 0.84 | 1.20 | 1.13 | 0.48 | -0.06 | 0 | 1 | 1 | . In order to compare the performance of the two implementations, we&#39;ll show a cross-tabulation for their aggregated correct and incorrect predictions . The built-in version was faster than the manually implemented version. A little less than 4 percent of the 4,500 predictions of the test dataset differ between the manually implemented and built-in versions of the algorithm. The off-diagonal entries (in light yellow) indicate the mismatches in predictions made by the two versions. This difference is likely to primarily be attributed to source of randomness in the algorithm.i.e. the drawing of a bootstrap sample. Although speed no doubt favours the built-in version, this would suggest that atleast the main functionality of the Random Forest algorithm has been replicated in our manual implementation here. . Overview of Gradient Boosting . The goal of the gradient boosting algorithm is to iteratively learn how to make corrections for the previous mistakes it has made. It does this by training multiple decision trees (base learners) individually. . As the name suggests, it makes use of the gradient descent algorithm. In our manual implementation, we will aim to minimize the mean squared error (MSE) which is the squared difference between the true and predicted values of the dependent variable (or target, y). This is combined with boosting - the process of enhancing of the learning capability of a weak learner (single decision tree) by iteratively using multiple decision trees to produce a strong learner. . An overview of the working of this algorithm is as follows . Calculate an average of the target (dependent variable) of the training data. Consider this to be the initial prediction. | Calculate the residual between the training data target and the initial predictions. | Iterate over a pre-determined number of decision trees and train each tree on the training data with the residual as the target during the first iteration, the residual between the initial guess for the prediction (the average) and the true training data target is taken as the residual | during subsequent iterations, it is the residual updated during the previous iteration that will be used here | . | Use the trained decision tree to predict the residual | Update the residual by subtracting this prediction from it subtract the product of learning rate and the residual the learning rate will determine how fast the residual is minimized during each iteration | a larger learning rate might overshoot the minimum but could reach there faster, while a smaller learning rate won&#39;t miss the minimum but will take longer to get there | . | . | Finally, append each trained decisoin tree to an empty list in order to build up a lilst of trained decision trees | Call each trained deciison tree to make a prediction of the test (unseen) data. Similar to training, multiply each prediction with the learning rate. | Finally, add up the predictions made by each trained decision tree (in the previous step) and add the initial guess (the average target value from the training data). | Manual Implementation of Gradient Boosting . As we did with the custom RandomForest implementation, we will create a custom gradient boosting regressor class that will accept parameters for the maximum tree depth, number of decision trees that we want to use and the learning rate. The maximim depth will be passed to the individual weak learners that we train. As with the random forest implementation, additional hyperparameters could be passed on to the weak learners, for this implementation, but we&#39;ll just use the maximum tree depth. We will see how the learning rate will be used next. We&#39;re again inheriting from the BaseEstimator and RegressorMixin classes to make the class compatible with scikit-learn. . class CustomGradientBoostingRegressor(BaseEstimator, TransformerMixin): def __init__(self, n_estimators=100, max_depth=10, learning_rate=0.01): self.learning_rate = learning_rate self.n_estimators = n_estimators self.max_depth = max_depth . As part of training this going to start by calculating the average of the target (dependent variable) from the training data. We&#39;ll also calculate the residual associated with this initial prediction by subtracting the prediction from the true target value. Note that this prediction and target value refer to an array of the training data and not to a single value. As mentioned above, this residual will be iteratively updated by each decision tree&#39;s predictions. . Next, we&#39;ll iterate over all the base learners (decision trees), train each on the training data but this time with the residual as the target. The trained decision tree is then used to predict the residual. This decision tree&#39;s prediction of the residual is then subtracted from the initial prediction of the residual - this is the process of updating the residual with each decision tree, eventually minimizing it. This is how the algorithm learns from its mistakes. . When updating the residuals, we will not just subtract the residual itself from the initial residual. Instead, we will subtract the product of the learning rate and the current tree&#39;s predicted residual. This is analogous to gradient descent. There, the gradient was updated in per iteration and needed to be minimized. Here, the residual is updated using the predictions of each trained base learner (or, each trained decision tree). . def fit(self, X, y): # initial prediction self.y_pred_ = np.mean(y) # initial residual residuals_ = (y - self.y_pred_).tolist() # training base learners to minimize residual self.estimators_ = [] for _ in range(self.n_estimators): # train tree_ = DecisionTreeRegressor(max_depth=self.max_depth) tree_.fit(X, residuals_) # update residual y_pred_train = tree_.predict(X) residuals_ = ( np.array(residuals_) - (self.learning_rate * y_pred_train) ).tolist() # build-up list of trained base learners self.estimators_.append(tree_) return self . In order to make predictions on unseen data, we will add up the predictions produced by each trained estimator in the list. This is the same as the scaling of the each tree&#39;s prediction, using the learning rate, during training in the .fit() method. So far, we have just been working with the residual of the initial prediction (the constant, or average, prediction). It has been updated by iterating over each of the individual decision trees. So, we&#39;ll now add this constant (the average of the training data target) back to the final summed prediction to give our overall predictions for the test (unseen) data. . @staticmethod def single_estimator_prediction(X, learning_rate, estimator): &quot;&quot;&quot;Make a prediction with a single trained decision tree.&quot;&quot;&quot; return learning_rate * estimator.predict(X) def predict(self, X): # Make predictions with each trained decision tree preds = [ self.single_estimator_prediction(X, self.learning_rate, est) for est in self.estimators_ ] # add up all predictions estimator_preds_ = np.sum(preds, axis=0) # add average prediction of target (from training data) to summed predictions y_pred_overall = estimator_preds_ + self.y_pred_ return y_pred_overall . The final complete class to realize this implementation is shown below . class CustomGradientBoostingRegressor(BaseEstimator, RegressorMixin): def __init__(self, n_estimators=100, max_depth=10, learning_rate=0.01): self.learning_rate = learning_rate self.n_estimators = n_estimators self.max_depth = max_depth def fit(self, X, y): # initial prediction self.y_pred_ = np.mean(y) # initial residual residuals_ = (y - self.y_pred_).tolist() # training base learners to minimize residual self.estimators_ = [] for _ in range(self.n_estimators): # train tree_ = DecisionTreeRegressor(max_depth=self.max_depth) tree_.fit(X, residuals_) # update residual y_pred_train = tree_.predict(X) residuals_ = ( np.array(residuals_) - (self.learning_rate * y_pred_train) ).tolist() # build-up list of trained base learners self.estimators_.append(tree_) return self @staticmethod def single_estimator_prediction(X, learning_rate, estimator): &quot;&quot;&quot;Make a prediction with a single trained decision tree.&quot;&quot;&quot; return learning_rate * estimator.predict(X) def predict(self, X): # Make predictions with each trained decision tree executor = Parallel(n_jobs=cpu_count(), backend=&#39;multiprocessing&#39;) tasks = ( delayed(self.single_estimator_prediction)(X, self.learning_rate, est) for est in self.estimators_ ) preds = executor(tasks) # add up all predictions estimator_preds_ = np.sum(preds, axis=0) # add average prediction of target (from training data) to summed predictions y_pred_overall = estimator_preds_ + self.y_pred_ return y_pred_overall . A change is made to the .predict() method in order to parallelize predictions made with each trained decision tree. When summing these predictions, we don&#39;t need them to be in a particular order so we can parallelize their calls to make a prediction. This parallelization step is the same as what we did above with the Random Forest implementation. . Using the custom gradient boosting implementation for regression . We can use the custom implementation to make predictions on a synthetic dataset. We can then compare these predictions to those made by the version of this algorithm built in to scikit-learn. To do this we&#39;ll generate a synthetic regression dataset of 20,000 rows and 25 features, of which 15 features will be useful for predicting the target . X, y = make_regression( n_samples=n_samples_gb, n_features=n_feats_gb, n_informative=n_informative_feats_gb, noise=noise_gb, ) df_X = pd.DataFrame(X, columns=[f&quot;var_{f_i}&quot; for f_i in range(1, n_feats_gb+1)]) s_y = pd.Series(y, name=&quot;target&quot;) . Recall that the properties of this synthetic regression dataset were defined earlier . n_samples_gb = 20000, n_feats_gb = 25, n_informative_feats_gb = 15, noise_gb = 25 . The first 5 rows of the generated dataset are shown below . First ten rows of independent (var_*) and dependent (target) variables in the data &nbsp; var_1 var_2 var_3 var_4 var_5 var_6 var_7 var_8 var_9 var_10 var_11 var_12 var_13 var_14 var_15 var_16 var_17 var_18 var_19 var_20 var_21 var_22 var_23 var_24 var_25 target . 0 -0.223011 | 0.179868 | -0.154820 | -2.227085 | 1.969173 | -0.060686 | -0.046739 | 0.784329 | 0.584825 | 0.234738 | 1.061733 | -0.676085 | -0.041723 | 1.634150 | -0.216616 | -1.597703 | -1.098633 | 0.917189 | 0.014585 | -1.157711 | -0.891429 | -0.795820 | 0.020996 | -0.194969 | -1.761135 | -158.264365 | . 1 -0.286999 | 0.593028 | -0.473005 | -1.165490 | -1.229666 | -0.091609 | 0.018704 | 2.236377 | -0.286575 | -1.425109 | -0.228449 | -0.184101 | -2.728834 | -0.276689 | -0.383918 | -0.508499 | 0.099501 | -0.107763 | 0.041479 | 1.295804 | -0.252423 | -1.057466 | -1.779850 | -0.355574 | -1.451316 | -333.507854 | . 2 -1.317927 | 0.283843 | -0.934341 | 0.097839 | -0.610285 | -1.511026 | 0.167301 | 1.093614 | -0.775090 | -0.940465 | -1.020371 | -1.319924 | 1.272913 | 0.640726 | -1.514854 | -0.724081 | -2.181743 | 1.216377 | -0.026584 | 0.803912 | 0.658917 | -0.906307 | 0.769189 | -0.954964 | -0.931621 | -82.140421 | . 3 1.052038 | 1.219628 | 0.472396 | -1.559922 | -2.413872 | 1.581726 | -0.004788 | 0.147760 | -0.969217 | 0.612192 | -1.276852 | -1.186855 | -0.757354 | -0.671043 | -1.010746 | 0.443628 | 0.488247 | -0.780966 | 1.066491 | 1.094533 | -0.234753 | -0.352541 | -0.414543 | -0.851234 | 1.088289 | -241.280018 | . 4 0.956539 | 0.105641 | -1.879069 | -0.902058 | -0.103156 | -0.494228 | 3.192277 | 1.894483 | 0.553269 | 0.163344 | -0.254190 | -1.358919 | -1.150939 | 1.061961 | -0.902114 | -0.086117 | 0.288102 | -0.512966 | 1.563868 | 0.878564 | 0.698072 | -0.783779 | 0.982344 | 0.505974 | -1.053844 | 396.099934 | . Splitting the data . As with the Random Forest usage above, we will divide this synthetic dataset into a training and testing split . X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30) . Pre-Processing . In order to explore correlation between features of the data, a heatmap of the feature-to-feature correlations is shown below . We&#39;ll assume we are looking to interpret the importance of features, even though our implementation does not currently support this, after predictions are made. So, we&#39;ll want to remove correlated features from this dataset before passing it to the gradient boosting algorithm. Luckily, our synthetic dataset doesn&#39;t have a problem of correlation between features. . Additionally, since tree-based models are not sensitive to the scale of the features, scaling of features to bring them onto the same scale is not required. However, using features on a different scale makes their interpretation difficult so we&#39;ll standardize all the features in the data. . Comparison of Predictions to Built-in Implementation of Gradient Boosting . A two-step pipeline with standardization before applying our custom gradient boosting implementation is defined below and trained on our training dataset before making predictions on the test data . %%time pipe_manual = Pipeline( [ (&quot;ss&quot;, StandardScaler()), (&quot;reg&quot;, CustomGradientBoostingRegressor(250, 7, 0.05)), ] ) pipe_manual.fit(X_train, y_train) y_pred_manual = pipe_manual.predict(X_test) . CPU times: user 45.6 s, sys: 56.2 ms, total: 45.6 s Wall time: 45.7 s . We now repeat the above step but this time we will use scikit-learn&#39;s built-in gradient boosting implementation, with the same hyper-parameters as those that we used in our custom implementation . %%time pipe_builtin = Pipeline( [ (&quot;ss&quot;, StandardScaler()), (&quot;reg&quot;, GradientBoostingRegressor(n_estimators=250, max_depth=7, learning_rate=0.05)), ] ) pipe_builtin.fit(X_train, y_train) y_pred_builtin = pipe_builtin.predict(X_test) . CPU times: user 46.9 s, sys: 2.64 ms, total: 46.9 s Wall time: 46.9 s . We&#39;ll now compare these two implementations in terms of their R^2, MSE, RMSE and MAE scores on the test set, by visualizing these metrics as a percent difference . or as a ratio to the built-in implementation . For the plot of percent differences, the subplot titles show the percent difference in each scoring metric, calculated on the testing data, relative to the manual implementation. So, a percent difference of -0.2 percent would mean that the built-in implementation is 0.2 percent smaller relative to the manual implementation. Encouragingly, there appears to be good agreement between the two versions here. . Either plot of the metrics suggests our manual impementation has adequately captured the core principles of the algorithm. . Also, the two implementations took approximately the same time (in seconds) to run to completion. The built-in version performs several sanity checks on the data, unlike the manual implementation, so this preliminary agreement in execution timing with the manual approach points to its impressive performance. . Summary and Additional Resources . We have implemented the core functionality of two decision tree-based ensembling algorithms for machine learning tasks. The ensembling method was unique for each algorithm but the outputs were in good aggreement with the implementations provided by the scikit-learn library. . Parallelization was provided for both algorithms&#39; predictions. While the total duration (training and prediction combined) of the manual Gradient Boosting implementation was comparable to that of the version provided by scikit-learn, future work on the Random Forest implementation should focus on improving the end-to-end duration of the training. . The versions offered by scikit-learn expose hyper-parameters of individual decision trees than can also be adjusted beyond the small selection of these that our manual implementation has exposed to the user. In gradient boosting, it is possible to use a sample of the training data for training each weak learner, thus following stochastic gradient descent rather than the batch-based approach that we used here. Early stopping can also be done by setting aside a fraction of the training data as a validation split. The scikit-learn version of the Random Forest classifier allows for weights to be assigned to the classes, which is one viable approach to handle imbalanced data. In both techniques scikit-learn allows the splitting criterion for an individual decision tree (eg. gini for classification, MSE for regression) to be adjusted. . Information about additional ensembling techniques, and how they can be used in other machine learning tasks, is available here (1, 2). .",
            "url": "https://elsdes3.github.io/analytics-ops-implementation-overload/random-forest/gradient-boosting/ml-algorithm-from-scratch/machine-learning/python/2021/11/06/ensembles.html",
            "relUrl": "/random-forest/gradient-boosting/ml-algorithm-from-scratch/machine-learning/python/2021/11/06/ensembles.html",
            "date": " • Nov 6, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Manually Implement K-Nearest Neighbours (KNN) from Scratch",
            "content": "Background . The K-Nearest Neighbours (KNN) algorithm is a statistical technique for finding the k samples in a dataset that are closest to a new sample that is not in the data. The algorithm can be used in both classification and regression tasks. In order to determine the which samples are closest to the new sample, the Euclidean distance is commonly used. KNN has been used in machine learning in some computer vision tasks such as recognizing hand-written numbers. . Goal . Here, we will go through the manual implementation of this algorithm using Python. Then, we&#39;ll use it to perform binary machine learning (ML) classification on a synthetic dataset. Finally, we will compare the performance of this manual implementation of the algorithm to the version that is provided by the scikit-learn Python library. . Overview of how the KNN algorithm works for ML binary classification . Since the KNN algorithm will be used in an ML classification context, we will make the following assumptions . we can divide our data into two splits, a training split and a testing split for each row (observation, or X matrix) in the training split, we have the associated class label (or y) | class labels are not required for observations in the test split | . | for simplicity, we will be working with a binary classification problem so our data will only have two labels (0, or False, and 1, or True) | . With the above assumption in mind, below is an outline of the working of KNN for ML classification . We start by calculating the Euclidean distance between each new observation (test data) that is not part of the training data and the training data. In an ML workflow, this means our ML model training will not actually perform any operations since the distance calculation is only left to the prediction phase when we have access to these new observations. | For each point in the new observations, we sort the distances to all the points in the training data. | Get the k shortest distances from a given point in the test data to every point in the training data. | From these k distances for a single test data point, get the corresponding k training data points. | Per our assumptions, we have class labels for all training data points. So, we will assign a label to a given test data point based on the most common label in the k closest training data points found in 4. above. As an example if k is 3 then and two of the three closest training data points (to a given test data point) have a class label 1, then the predicted class label for that test data point will also be 1. | Manual Implementation of KNN . In order to implement this algorithm, we&#39;ll create a custom class which accepts only one input parameter - the number of nearest neighbours (k) - in its __init__ method. We&#39;ll be following the scikit-learn API, so we&#39;ll inherit from scikit-learn&#39;s BaseEstimator and, since we will be using this with a classification problem, ClassifierMixin base classes. . class CustomKNN(BaseEstimator, ClassifierMixin): def __init__(self, k=5): self.k = k . As mentioned above, explicit training is not required for KNN. So, there is nothing to do during training of the algorithm. However, we do want to follow the assumption we made earlier that our training data is clearly separated from our testing data. Since only the training data is used during ML model training, we&#39;ll assign the data that can be accessed by the .fit() method (i.e. ML model training) to be the training observations (X_train) and training class labels (y_train). This way, when using the KNN algorithm to make predictions on new observations, we can refer back to these variables (X_train and y_train) and know that they exclude the new observations for which we want to predict class labels. These two variables will only exist after the .fit() method has been called, so trying to make predictions without first training the ML model (in this case, identifying the training data) will fail. . def fit(self, X, y): self.X_train = X self.y_train = y . It is during prediction of the class labels that the KNN algorithm does its work. So, in our class&#39; .predict() method, we&#39;ll implement the above details of this algorithm. We&#39;ll iterate over each new (test) data point and then call a helper function make_single_prediction() that does the following . calculate Eulidean distance between the selected test data point and all training data points | find the k shortest distances | get the corresponding training data class labels for each of the k shortest distances these will be the labels for the k training data points closest to the selected test data point | . | return the most commonly occuring training data label, among the k closest training data points, as the predicted label for the selected test data point | This process will be repeated for all other test data points. . The Python code to implement this is shown below, with comments indicating the code for each of these four steps . def get_euclidean_distance(array_1, array_2): # built-in approach dist = DistanceMetric.get_metric(&quot;euclidean&quot;) X = [array_1, array_2] dist_matrix = dist.pairwise(X) # # Manual approach # dist_manual = np.sqrt(np.sum((array_1 - array_2) ** 2)) # assert dist_manual == dist_matrix[0, 1] return dist_matrix[0, 1] def use_probabilities_to_get_labels(y_train_labels, threshold=0.5): y_pred_proba = np.mean(y_train_labels) y_pred = 0 if y_pred_proba &lt; threshold else 1 return y_pred @staticmethod def make_single_prediction(X_test, X_train, y_train, k): # 1. Compute distances between x and all examples in the training set distances = [get_euclidean_distance(X_test, X_train) for X_train in X_train] # 2. Sort by distance and return indices of the first k neighbors k_idx = np.argsort(distances)[:k] # 3. Extract the labels of the k nearest neighbor training samples # y_train_k_neighbour_labels = [y_train[i] for i in k_idx] y_train_k_neighbour_labels = y_train[k_idx] # 4. return the most common class label # # without using probabilities y_pred = Counter(y_train_k_neighbour_labels).most_common(1)[0][0] # # (optional) using probabilities y_pred = use_probabilities_to_get_labels(y_train_k_neighbour_labels, 0.5) return y_pred def predict(self, X): y_pred = np.array( [ self.make_single_prediction(obs, self.X_train, self.y_train, self.k) for obs in X ] ) return y_pred . where . get_euclidean_distance() calculate Euclidean distance between two points) | . | use_probabilities_to_get_labels() convert prediction probabilities into hard labels using a user-adjustable discrimination threshold of 0.5 | . | . are standalone Python functions while make_single_prediction() is a static method (1, 2) for our CustomKNN class. . The final custom KNN class and commented versions of the two standalone functions are shown below . def get_euclidean_distance(array_1, array_2): # built-in approach dist = DistanceMetric.get_metric(&quot;euclidean&quot;) X = [array_1, array_2] dist_matrix = dist.pairwise(X) # # Manual approach # dist_manual = np.sqrt(np.sum((array_1 - array_2) ** 2)) # assert dist_manual == dist_matrix[0, 1] return dist_matrix[0, 1] def use_probabilities_to_get_labels(y_train_labels, threshold=0.5): # get average of training labels y_pred_proba = np.mean(y_train_labels) # assign test labels by comparing average label value to threshold y_pred = 0 if y_pred_proba &lt; threshold else 1 return y_pred . class CustomKNN(BaseEstimator, ClassifierMixin): def __init__(self, k=5, threshold=None): self.k = k self.threshold = threshold def fit(self, X, y): self.X_train = X self.y_train = y @staticmethod def make_single_prediction(test_obs_indx, X_test_obs, X_train, y_train, k, threshold=None): # 1. Compute distances between x and all examples in the training set distances = [get_euclidean_distance(X_test_obs, X_train) for X_train in X_train] # 2. Sort by distance and return indices of the first k neighbors k_idx = np.argsort(distances)[:k] # 3. Extract the labels of the k nearest neighbor training samples y_train_k_neighbour_labels = y_train[k_idx] # 4. return the most common class label if not threshold or len(set(y_train)) &gt; 2: # without using probabilities (binary or multi-class) y_pred = Counter(y_train_k_neighbour_labels).most_common(1)[0][0] else: # using probabilities (binary only) y_pred = use_probabilities_to_get_labels( y_train_k_neighbour_labels, threshold ) return [test_obs_indx, y_pred] def predict(self, X): executor = Parallel(n_jobs=cpu_count(), backend=&#39;multiprocessing&#39;) tasks = ( delayed(self.make_single_prediction)( idx, test_obs, self.X_train, self.y_train, self.k ) for idx, test_obs in enumerate(X) ) y_pred = np.array(executor(tasks)) # sort by index (first column) to ensure same index as X, and # get extract second column (labels) y_pred = y_pred[np.argsort(y_pred[:, 0])][:, 1] return y_pred . Two changes has been made to the above class . as an alternative to taking the most commonly occuring label among the the k closest training data labels, we could calculate the average value of the k training labels and compare this to a threshold (for example, 0.5). If the average value is less than 0.5 then we will assign 0, otherwise we&#39;ll asisgn 1. in the class, this is done if a threshold is specified as the threshold input parameter | . | the iteration over each observation in the test data is parallelized to reduce the time required to run the algorithm | . Using the manual KNN implementation with data . In order to compare this algorithm to the version provided by scikit-learn, we&#39;ll generate 15,000 observations of synthetic classification data with scikit-learn&#39;s make_classification() data generator. We&#39;ll specify that 10 columns are to be creaed, nine of which are useful for predecting the binary class labels . nrows = 15_000 n_feats = 10 n_informative_feats = 9 noise_factor = 30 flip_y = 0.4 . X, y = make_classification( n_samples=nrows, n_features=n_feats, n_informative=n_informative_feats, n_redundant=0, n_classes=2, flip_y=flip_y, # values closer to 1.0 increase difficulty of classification n_clusters_per_class=1, class_sep=1.0, random_state=0, ) df_X = pd.DataFrame(X, columns=[f&quot;var_{f_i}&quot; for f_i in range(1, 10+1)]) s_y = pd.Series(y, name=&quot;target&quot;) . The first 10 rows of the generated dataset are shown below . First ten rows of independent (var_*) and dependent (target) variables in the data &nbsp; var_1 var_2 var_3 var_4 var_5 var_6 var_7 var_8 var_9 var_10 target . 0 -1.351 | -2.093 | 0.975 | 2.314 | -0.542 | 3.762 | -2.942 | -0.499 | -0.676 | 2.150 | 0 | . 1 -2.368 | 0.212 | 2.423 | 0.135 | 0.249 | -3.654 | -0.815 | 1.223 | 0.934 | -0.505 | 0 | . 2 0.039 | 0.269 | -0.502 | 0.205 | -0.251 | 0.290 | -2.112 | 0.963 | -1.284 | 2.250 | 1 | . 3 -1.890 | -0.034 | -0.001 | 3.669 | -2.930 | 1.354 | 0.505 | -0.614 | -1.797 | -1.236 | 1 | . 4 -1.376 | -2.345 | -1.106 | 3.706 | 2.350 | 0.329 | 1.900 | 1.232 | -1.652 | -2.146 | 0 | . 5 -1.286 | -0.143 | -0.912 | 0.486 | -0.216 | 1.437 | 0.712 | 1.190 | 1.809 | -1.286 | 1 | . 6 -2.267 | -0.931 | 1.427 | 2.336 | -1.834 | -0.845 | -0.657 | -3.034 | 0.141 | 1.171 | 0 | . 7 -0.458 | 0.139 | 1.798 | 2.092 | 2.189 | 2.745 | -3.283 | -2.552 | -2.631 | 1.540 | 0 | . 8 -2.245 | 1.740 | -0.322 | -0.410 | 0.927 | -1.926 | 1.171 | 2.617 | 1.393 | -0.298 | 0 | . 9 -0.264 | 1.967 | -1.555 | 0.427 | 1.860 | -1.128 | -1.388 | -0.714 | -1.782 | 0.819 | 1 | . Create training and testing splits of the overall dataset . We will split the generated data into the required training and testing splits . X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30) . The classes in the training split from the synthetic dataset are balanced, as shown below . So, we don&#39;t need to re-balance the data before passing it to the Random Forest algorithm. . Feature Selection . Below is a heatmap of the feature-to-feature correlatoin between every pair of features in the synthetic data we are working with . There is some evidence of multi-feature correlation which will result in KNN not working properly. In a simple case of two uncorrelated features, each contributes equally to the distance calculation. If a third feature, that is correlated to one of the existing features, is added then this feature is almost a copy of one of the other two. A combination of the two correlated features will have a greater impact on the distance, which will negatively impact accuracy of the algorithm. . A general ML-related problem with multi-feature correlation, independent of KNN, is interpreting how such features impact the output and relate to eachother. . For these two reasons, we&#39;ll drop the var_2 which is correlated with var_4, var_7, var_8 and var_10. Also, var_6 appears to be correlated with a few features so it too can be dropped. . selected_cols = list(set(list(df_X)) - set([&quot;var_2&quot;, &quot;var_6&quot;])) X_train = pd.DataFrame(X_train, columns=list(df_X)).drop(columns=[&quot;var_2&quot;, &quot;var_6&quot;]).to_numpy() X_test = pd.DataFrame(X_test, columns=list(df_X)).drop(columns=[&quot;var_2&quot;, &quot;var_6&quot;]).to_numpy() . Pre-Processing and Making Predictions . For algorithms that depend on distance, such as KNN, including features on different scales (range from min to max) means they affect the distance differently. All the nearest neighbors (shortest distances) will be follow the feature with a smaller range, inflating their importance. To prevent this, we&#39;ll use a two-step pipeline to first normalize all features (placing them on a scale from 0 to 1) before passing them to the algorithm for classification . %%time pipe_manual = Pipeline( [ (&quot;ss&quot;, MinMaxScaler()), (&quot;reg&quot;, CustomKNN(k=5, threshold=0.5)), ] ) pipe_manual.fit(X_train, y_train) y_pred_manual = pipe_manual.predict(X_test) . CPU times: user 8.79 s, sys: 870 ms, total: 9.66 s Wall time: 49.6 s . Here, the algorithm is trained on the normalized training data and then makes predictions on the testing data. . Comparison of Predictions to Built-in Implementation . We&#39;ll again apply normalization with the built-in KNN implementation from scikit-learn and make predictions on the testing data . %%time pipe_builtin = Pipeline( [ (&quot;ss&quot;, MinMaxScaler()), (&quot;reg&quot;, KNeighborsClassifier(n_neighbors=5)), ] ) pipe_builtin.fit(X_train, y_train) y_pred_builtin = pipe_builtin.predict(X_test) . CPU times: user 264 ms, sys: 1.91 ms, total: 265 ms Wall time: 264 ms . The first and last five rows of the testing data, including the predicted labels with the manually implemented and built-in approaches are shown below . First and Last 5 predictions of test split &nbsp; var_3 var_10 var_7 var_8 var_4 var_9 var_5 var_1 true manually_implemented_knn built_in_knn . 0 -1.69 | 1.55 | 1.26 | 1.64 | -0.82 | -1.61 | -0.94 | 0.62 | 0 | 0 | 0 | . 1 0.43 | 0.47 | 5.14 | 1.32 | -1.25 | 0.47 | -4.45 | -1.15 | 1 | 1 | 1 | . 2 -0.31 | -0.25 | 1.33 | -0.37 | -1.86 | -0.82 | -1.86 | 3.11 | 1 | 0 | 0 | . 3 -0.10 | 1.08 | 1.03 | 0.42 | -1.00 | -0.69 | -0.72 | 1.42 | 1 | 0 | 0 | . 4 -0.20 | -1.56 | 2.14 | -3.30 | -1.08 | 0.42 | -2.08 | 0.54 | 1 | 1 | 1 | . 4495 -1.35 | -1.63 | 1.76 | -4.26 | -2.49 | -0.28 | -4.27 | -0.56 | 0 | 0 | 0 | . 4496 -0.64 | -0.18 | -1.19 | 1.73 | -3.63 | 0.59 | -2.57 | 1.45 | 0 | 1 | 1 | . 4497 0.82 | -0.50 | -0.86 | -2.42 | -2.95 | -1.19 | -1.20 | 2.00 | 1 | 1 | 1 | . 4498 -1.85 | 0.92 | -0.17 | 1.84 | 0.29 | 3.89 | 1.18 | -2.13 | 0 | 0 | 0 | . 4499 -2.95 | 0.36 | 2.16 | 5.57 | -1.29 | 1.92 | 0.74 | -2.31 | 0 | 0 | 0 | . It is clear that neither implementation is perfectly predicting the true class labels of the test set. But, they do appear to be performing equivalently which is encouranging. Due to the absence of the for loop, the built-in approach is significantly faster. . We&#39;ll explicity verify that there is no observation where a different class is predicted using the manually implemented and built-in versions of the KNN algorithm . df_mismatched = df_pred_compare.query(&quot;manually_implemented_knn != built_in_knn&quot;) try: assert df_mismatched.empty print(&quot;Did not find mismatch between predictions of each implementation&quot;) except AssertionError as _: print(f&quot;Find {len(df_mismatched):,} mismatches between predictions of each implementation&quot;) . Did not find mismatch between predictions of each implementation . Below is a comparison of the number of predictions of each true label in the test set, and the predictions made with each version . Predicted labels by class &nbsp; true manual built-in . 0 2262 | 2320 | 2320 | . 1 2238 | 2180 | 2180 | . This is visually shown below . We can see that these aggregated counts of predictions for each label, with the manually implemented and built-in versions, agree with eachother. . Finally, we&#39;ll show a confusion matrix for each implementation . Confusion Matrix for manually implemented KNN Predicted class_label=0 class_label=1 . True &nbsp; &nbsp; . class_label=0 1698 | 564 | . class_label=1 622 | 1616 | . Confusion Matrix for built-in KNN Predicted class_label=0 class_label=1 . True &nbsp; &nbsp; . class_label=0 1698 | 564 | . class_label=1 622 | 1616 | . Visually, these confusion matrices are shown below . As we can see, for binary classification, the predictions of the manual KNN implementation match those of the version provided by scikit-learn. The off-diagonal entries (in light yellow) indicate the incorrect predictions made by the algorithm. Tuning the number of nearest neighbours to consider can possibly reduce (improve) these incorrect predictions. As the scope here was focused on implementation of the algorithm, and there is agreement between the counts in these two confusion matrices, this suggests agreement between the manual implementation and scikit-learn&#39;s built-in version. . Limitations and Resources . This was a walkthrough the implementation and use of the KNN algorithm from scratch. The focus here was on implementing the core logic of the algorithm. The version of the KNN classifier built in to scikit-learn offers additional functionality such as assigning the weights to points in a neighbourhood of a point of interest before selecting the k closest points. As al alternative to the brute force approach used here, for example, the Ball Tree algorithm for data partitioning to improve the computation speed of n-point problems can be used. . From a technical perspective, the implementation here is not ideal since it involves iterating over each observation in the test data leads to a slow computation time. The built-in implementation completed nearly instantly, but our manual implementation took nearly 45 seconds for this dataset of 15,000 rows. This problem only gets worse as the number of observations in the data increase. Multi-core parallelization provides some improvement, but, regrettably, the real increase in efficiency of this manual implementation comes from completely replacing the use of loops with vectorization using the numpy library in Python. This is done in the built-in version of this algorithm in scikit-learn (1). Future iterations of this manual implementation should focus on using vectorization to speed up the KNN model&#39;s predictions. .",
            "url": "https://elsdes3.github.io/analytics-ops-implementation-overload/ml-algorithm-from-scratch/knn/nearest-neighbours/machine-learning/python/2021/10/30/knn.html",
            "relUrl": "/ml-algorithm-from-scratch/knn/nearest-neighbours/machine-learning/python/2021/10/30/knn.html",
            "date": " • Oct 30, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Cohort Analysis for Retention of Customers",
            "content": "Background . Cohort analysis is a branch of behavioural analytics that congregates customer data before conducting analysis. The underlying principle is to consider a group, or cohort, of customers who exhibit some common attributes within a finite period of time. . The purpose of using this technique is to understand customer engagement over the chosen period of time. For example, it might allow for an understanding of whether the business is really better engaging users over time or if this is in fact only increasing due to growth. Indeed separating these two concepts - growth and true customer engagement - is a vital benefit of this technique. . One grouping of customers in cohort analysis can be done based on when they first made a transaction at a retail store. We could break down cohorts by, for example, the month, week or day they first made a transaction at a store, and thus track monthly, weekly or daily cohorts. With the logic for grouping users into a cohort defined, we can then measure the business&#39; retention of these cohorts to determine for how long customers continue to shop at a retail store relative to their first recorded transaction at that store. This is called an acquisition cohort and we&#39;ll be focusing on this here. . Objective . Here, we will focus on monthly user retention rate. We&#39;ll use Python to conduct cohort analysis on customer transaction data at an online retail store. . About the Customer Data . The dataset we&#39;ll use is taken from the UCI repository and covers transaction data for an online-only retail company. The data is provided in a single .xlsx file. . Get the Data . We&#39;ll download the data file to a file named online_retail.xlsx in the current working directory. . PROJ_ROOT_DIR = os.getcwd() . url = ( &quot;https://archive.ics.uci.edu/ml/machine-learning-databases/00352/&quot; &quot;Online%20Retail.xlsx&quot; ) d_weekdays = [ &quot;Monday&quot;, &quot;Tuesday&quot;, &quot;Wednesday&quot;, &quot;Thursday&quot;, &quot;Friday&quot;, &quot;Saturday&quot;, &quot;Sunday&quot;, ] raw_data_filepath = os.path.join( PROJ_ROOT_DIR, os.path.basename(url).lower().replace(&quot;%20&quot;, &quot;_&quot;) ) . Load Data . We&#39;ll now load the data into a DataFrame while parsing the InvoiceDate column as a datetyime format . dtypes_dict = { &quot;Description&quot;: pd.StringDtype(), &quot;StockCode&quot;: pd.StringDtype(), &quot;Country&quot;: pd.StringDtype(), &quot;CustomerID&quot;: pd.StringDtype(), &quot;InvoiceNo&quot;: pd.StringDtype(), &quot;UnitPrice&quot;: pd.Float64Dtype(), &quot;Quantity&quot;: pd.Float64Dtype(), } . %%time df = pd.read_excel( raw_data_filepath, dtype=dtypes_dict, parse_dates=[&quot;InvoiceDate&quot;], ) display( df.head() .append(df.tail()) .style.format(&quot;{:,.3f}&quot;, subset=[&quot;UnitPrice&quot;]) .format(&quot;{:,.0f}&quot;, subset=[&quot;Quantity&quot;]) .set_caption(&quot;First and last 5 rows&quot;) ) . First and last 5 rows &nbsp; InvoiceNo StockCode Description Quantity InvoiceDate UnitPrice CustomerID Country . 0 536365 | 85123A | WHITE HANGING HEART T-LIGHT HOLDER | 6 | 2010-12-01 08:26:00 | 2.550 | 17850 | United Kingdom | . 1 536365 | 71053 | WHITE METAL LANTERN | 6 | 2010-12-01 08:26:00 | 3.390 | 17850 | United Kingdom | . 2 536365 | 84406B | CREAM CUPID HEARTS COAT HANGER | 8 | 2010-12-01 08:26:00 | 2.750 | 17850 | United Kingdom | . 3 536365 | 84029G | KNITTED UNION FLAG HOT WATER BOTTLE | 6 | 2010-12-01 08:26:00 | 3.390 | 17850 | United Kingdom | . 4 536365 | 84029E | RED WOOLLY HOTTIE WHITE HEART. | 6 | 2010-12-01 08:26:00 | 3.390 | 17850 | United Kingdom | . 541904 581587 | 22613 | PACK OF 20 SPACEBOY NAPKINS | 12 | 2011-12-09 12:50:00 | 0.850 | 12680 | France | . 541905 581587 | 22899 | CHILDREN&#39;S APRON DOLLY GIRL | 6 | 2011-12-09 12:50:00 | 2.100 | 12680 | France | . 541906 581587 | 23254 | CHILDRENS CUTLERY DOLLY GIRL | 4 | 2011-12-09 12:50:00 | 4.150 | 12680 | France | . 541907 581587 | 23255 | CHILDRENS CUTLERY CIRCUS PARADE | 4 | 2011-12-09 12:50:00 | 4.150 | 12680 | France | . 541908 581587 | 22138 | BAKING SET 9 PIECE RETROSPOT | 3 | 2011-12-09 12:50:00 | 4.950 | 12680 | France | . CPU times: user 1min 4s, sys: 252 ms, total: 1min 4s Wall time: 1min 19s . Each transaction covers a certain number of products. This quantity and the unit price of each product are available in the data. A text-based description of the order and the invoice date and time are provided. We won&#39;t be working with the time here so we&#39;ll only consider the date component of the InvoiceDate column. . Strictly for the purposes of cohort analysis of customer retention, we only require the InvoiceDate and CustomerID columns. But, in order to the clean the dataset (to be done next), we&#39;ll need to consider a subset of the other columns in the raw data. . The range of dates covering these transactions is shown below . Raw data includes 541,909 transactions, covering 2010-12-01 to 2011-12-09. . We have 13 months of the transaction data for this online retailer. . Data Cleaning . We&#39;ll perform some basic cleaning of the two quantitative columns in the dataset. Descriptive statistics for these are shown below . Statistics about raw data &nbsp; Quantity UnitPrice . count 541,909.000 | 541,909.000 | . mean 9.552 | 4.611 | . std 218.081 | 96.760 | . min -80,995.000 | -11,062.060 | . 25% 1.000 | 1.250 | . 50% 3.000 | 2.080 | . 75% 10.000 | 4.130 | . max 80,995.000 | 38,970.000 | . There are transactions with negative values in both of these columns. These could be transactions that resulted in a product return. . We&#39;ll extract the invoice numbers for the transactions that were returns . returns = df.query(&quot;Quantity &lt; 0&quot;) . The first five returns are shown below . &nbsp; InvoiceNo StockCode Description Quantity InvoiceDate UnitPrice CustomerID Country . 141 C536379 | D | Discount | -1 | 2010-12-01 09:41:00 | 27.500 | 14527 | United Kingdom | . 154 C536383 | 35004C | SET OF 3 COLOURED FLYING DUCKS | -1 | 2010-12-01 09:49:00 | 4.650 | 15311 | United Kingdom | . 235 C536391 | 22556 | PLASTERS IN TIN CIRCUS PARADE | -12 | 2010-12-01 10:24:00 | 1.650 | 17548 | United Kingdom | . 236 C536391 | 21984 | PACK OF 12 PINK PAISLEY TISSUES | -24 | 2010-12-01 10:24:00 | 0.290 | 17548 | United Kingdom | . 237 C536391 | 21983 | PACK OF 12 BLUE PAISLEY TISSUES | -24 | 2010-12-01 10:24:00 | 0.290 | 17548 | United Kingdom | . We&#39;ll also count missing values in the full dataset and view a single sample value in each column. These are shown below . Summarizing string dtype columns &nbsp; max_value dtype num_missing single_non_nan_value . CustomerID 18287 | string | 135080 | 15498 | . InvoiceNo C581569 | string | 0 | 562939 | . UnitPrice 38970.000000 | Float64 | 0 | 1.250000 | . StockCode m | string | 0 | 23077 | . Description wrongly sold sets | string | 1454 | DOUGHNUT LIP GLOSS | . Quantity 80995.000000 | Float64 | 0 | 20.000000 | . InvoiceDate 2011-12-09 12:50:00 | datetime64[ns] | 0 | 2011-08-11 08:37:00 | . Country Unspecified | string | 0 | United Kingdom | . Summarizing Float64 dtype columns &nbsp; max_value dtype num_missing single_non_nan_value . CustomerID 18287 | string | 135080 | 14217 | . InvoiceNo C581569 | string | 0 | 565125 | . UnitPrice 38970.000000 | Float64 | 0 | 4.150000 | . StockCode m | string | 0 | 23075 | . Description wrongly sold sets | string | 1454 | PARLOUR CERAMIC WALL HOOK | . Quantity 80995.000000 | Float64 | 0 | 4.000000 | . InvoiceDate 2011-12-09 12:50:00 | datetime64[ns] | 0 | 2011-09-01 12:16:00 | . Country Unspecified | string | 0 | United Kingdom | . Column Datatypes and Missing Values &nbsp; num_missing dtype . InvoiceNo 0 | string | . StockCode 0 | string | . Description 1454 | string | . Quantity 0 | Float64 | . InvoiceDate 0 | datetime64[ns] | . UnitPrice 0 | Float64 | . CustomerID 135080 | string | . Country 0 | string | . Since the CustomerID column will be needed later, we can&#39;t keep transactions without a value in this field. So, we&#39;ll drop such transactions and re-display the descriptive statistics . Statistics about raw data &nbsp; Quantity UnitPrice . count 406,829.000 | 406,829.000 | . mean 12.061 | 3.460 | . std 248.693 | 69.315 | . min -80,995.000 | 0.000 | . 25% 2.000 | 1.250 | . 50% 5.000 | 1.950 | . 75% 12.000 | 3.750 | . max 80,995.000 | 38,970.000 | . The negative UnitPrices have been removed but negative Quantity values remain. We&#39;ll exclude the transactions with a negative Quantity . df = df.dropna(subset=[&quot;CustomerID&quot;]).query(&quot;Quantity &gt; 0&quot;) . The descriptive statistics of the cleaned dataset are shown below . Statistics about cleaned data &nbsp; Quantity UnitPrice . count 397,924.000 | 397,924.000 | . mean 13.022 | 3.116 | . std 180.420 | 22.097 | . min 1.000 | 0.000 | . 25% 2.000 | 1.250 | . 50% 6.000 | 1.950 | . 75% 12.000 | 3.750 | . max 80,995.000 | 8,142.750 | . The number of missing values and a sample value for each column in the cleaned dataset are again displayed . Summarizing string dtype columns &nbsp; max_value dtype num_missing single_non_nan_value . CustomerID 18287 | string | 0 | 14096 | . InvoiceNo 581587 | string | 0 | 572552 | . UnitPrice 8142.750000 | Float64 | 0 | 2.920000 | . StockCode POST | string | 0 | 84988 | . Description ZINC WIRE SWEETHEART LETTER TRAY | string | 0 | SET OF 72 PINK HEART PAPER DOILIES | . Quantity 80995.000000 | Float64 | 0 | 1.000000 | . InvoiceDate 2011-12-09 12:50:00 | datetime64[ns] | 0 | 2011-10-24 17:07:00 | . Country Unspecified | string | 0 | United Kingdom | . Summarizing Float64 dtype columns &nbsp; max_value dtype num_missing single_non_nan_value . CustomerID 18287 | string | 0 | 13488 | . InvoiceNo 581587 | string | 0 | 547580 | . UnitPrice 8142.750000 | Float64 | 0 | 2.950000 | . StockCode POST | string | 0 | 22699 | . Description ZINC WIRE SWEETHEART LETTER TRAY | string | 0 | ROSES REGENCY TEACUP AND SAUCER | . Quantity 80995.000000 | Float64 | 0 | 6.000000 | . InvoiceDate 2011-12-09 12:50:00 | datetime64[ns] | 0 | 2011-03-24 10:48:00 | . Country Unspecified | string | 0 | United Kingdom | . Column Datatypes and Missing Values &nbsp; num_missing dtype . InvoiceNo 0 | string | . StockCode 0 | string | . Description 0 | string | . Quantity 0 | Float64 | . InvoiceDate 0 | datetime64[ns] | . UnitPrice 0 | Float64 | . CustomerID 0 | string | . Country 0 | string | . We&#39;ll use this cleaned version of the dataset for further analysis. . Briefly exploring the dataset, we can show the number of transactions by hour of day and day of the week . weekday Monday Tuesday Wednesday Thursday Friday Saturday Sunday . inv_hour . 0 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 1 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 2 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 3 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 4 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 5 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 6 0 | 0 | 0 | 1 | 0 | 0 | 0 | . 7 61 | 65 | 59 | 64 | 130 | 0 | 0 | . 8 1564 | 1711 | 1818 | 1765 | 1833 | 0 | 0 | . 9 4348 | 4546 | 4078 | 4572 | 4376 | 0 | 25 | . 10 6132 | 6615 | 6944 | 7548 | 7223 | 0 | 3537 | . 11 7506 | 8495 | 8389 | 7186 | 7299 | 0 | 10217 | . 12 10731 | 12036 | 12932 | 13056 | 9888 | 0 | 13426 | . 13 10596 | 10429 | 11321 | 11595 | 8417 | 0 | 11673 | . 14 9213 | 9614 | 8832 | 9260 | 7329 | 0 | 9879 | . 15 7705 | 7299 | 7855 | 7732 | 4726 | 0 | 10055 | . 16 4065 | 3633 | 4223 | 6098 | 2111 | 0 | 3963 | . 17 2978 | 1964 | 2404 | 4323 | 1403 | 0 | 0 | . 18 0 | 69 | 0 | 2858 | 2 | 0 | 0 | . 19 0 | 0 | 15 | 3218 | 89 | 0 | 0 | . 20 0 | 0 | 18 | 776 | 8 | 0 | 0 | . 21 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 22 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 23 0 | 0 | 0 | 0 | 0 | 0 | 0 | . Although this is an online retailer, they strangely don&#39;t receive purchase transactions during early hours of the morning or late hours of the night, or even on Saturdays. . Extracting Variables to Define Monthly Customer Retention . We&#39;ll append new columns to the dataset for . the date (year and month) of the first transaction for each customer since we&#39;re looking at the monthly cohort, we only need the month of the date and not the day of the month | . | the current transaction&#39;s year and month | . df[&quot;FirstInvoiceDate&quot;] = ( df.groupby(&quot;CustomerID&quot;)[&quot;InvoiceDate&quot;].transform(&quot;min&quot;).dt.strftime(&quot;%Y-%m&quot;) ) df[&quot;Invoice_YearMonth&quot;] = df[&quot;InvoiceDate&quot;].dt.strftime(&quot;%Y-%m&quot;) . Both of these columns will be used for analysis in the next step. The FirstInvoiceDate represents cohorts of customers who all performed their first transaction during the same month in the same year. . Data Transformation . We&#39;ll now get the date of the first transaction performed by a given cohort and the date of the every transaction. Since the first transaction also falls under every transaction, the first row of this combination of date columns will be the same. . To do this, we&#39;ll count the number of customers who made transactions for every grouping of the first invoice date (FirstInvoiceDate) and invoice date (Invoice_YearMonth). This is done below . cohorts = ( df.groupby([&quot;FirstInvoiceDate&quot;, &quot;Invoice_YearMonth&quot;], as_index=False)[&quot;CustomerID&quot;] .nunique() .rename(columns={&quot;CustomerID&quot;: &quot;num_customers&quot;}) ) . . and the first and last five rows of the output are shown below . First &amp; Last 5 rows &nbsp; FirstInvoiceDate Invoice_YearMonth num_customers . 0 2010-12 | 2010-12 | 885 | . 1 2010-12 | 2011-01 | 324 | . 2 2010-12 | 2011-02 | 286 | . 3 2010-12 | 2011-03 | 340 | . 4 2010-12 | 2011-04 | 321 | . 86 2011-10 | 2011-11 | 86 | . 87 2011-10 | 2011-12 | 41 | . 88 2011-11 | 2011-11 | 324 | . 89 2011-11 | 2011-12 | 36 | . 90 2011-12 | 2011-12 | 41 | . This has transformed the data into an aggregated form by year-month date rather than keeping it on a per-transaction basis. . The reason for doing this is to calculate the time difference (in months) between each cohort&#39;s first and all transaction dates. The above data transformation facilitates this calculation. . As mentioned above, for a cohort, the first transaction date also appears as one of the dates of all the transactions. For this reason, the first row of each cohort always has the same date in the FirstInvoiceDate and Invoice_YearMonth columns and, as we&#39;ll see next, the time difference will be zero months for this row. . Within each grouping above, we will append a column with the difference in months (1) between the first and every transaction date . cohorts[&quot;First_to_Current_InvoiceMonth&quot;] = ( pd.to_datetime(cohorts[&quot;Invoice_YearMonth&quot;]).dt.to_period(&quot;M&quot;) - pd.to_datetime(cohorts[&quot;FirstInvoiceDate&quot;]).dt.to_period(&quot;M&quot;) ).apply(attrgetter(&quot;n&quot;)) . The first and last five rows of the transformed data, including this appended column, are shown below . First &amp; Last 5 rows &nbsp; FirstInvoiceDate Invoice_YearMonth num_customers First_to_Current_InvoiceMonth . 0 2010-12 | 2010-12 | 885 | 0 | . 1 2010-12 | 2011-01 | 324 | 1 | . 2 2010-12 | 2011-02 | 286 | 2 | . 3 2010-12 | 2011-03 | 340 | 3 | . 4 2010-12 | 2011-04 | 321 | 4 | . 86 2011-10 | 2011-11 | 86 | 1 | . 87 2011-10 | 2011-12 | 41 | 2 | . 88 2011-11 | 2011-11 | 324 | 0 | . 89 2011-11 | 2011-12 | 36 | 1 | . 90 2011-12 | 2011-12 | 41 | 0 | . As expected, the first row of every grouping is zero. For every cohort which only had transactions in a single month of a single year, the only time difference is zero months (see the last row of the above output). . Customer Retention Matrix . Finally, we&#39;ll re-shape this aggregated version of the data to have the first invoice date as the rows and the average transaction price for all subsequent transactions, by month, in the columns. This is known as a retention matrix and is done below . df_cohort_pivotted = cohorts.pivot_table( index=&quot;FirstInvoiceDate&quot;, columns=&quot;First_to_Current_InvoiceMonth&quot;, values=&quot;num_customers&quot;, ) df_cohort_pivotted = df_cohort_pivotted.add_prefix(&quot;Month &quot;) . This re-shaped version of the aggregated data is shown below . First and last 5 rows First_to_Current_InvoiceMonth Month 0 Month 1 Month 2 Month 3 Month 4 Month 5 Month 6 Month 7 Month 8 Month 9 Month 10 Month 11 Month 12 . FirstInvoiceDate &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; . 2010-12 885 | 324 | 286 | 340 | 321 | 352 | 321 | 309 | 313 | 350 | 331 | 445 | 235 | . 2011-01 417 | 92 | 111 | 96 | 134 | 120 | 103 | 101 | 125 | 136 | 152 | 49 | nan | . 2011-02 380 | 71 | 71 | 108 | 103 | 94 | 96 | 106 | 94 | 116 | 26 | nan | nan | . 2011-03 452 | 68 | 114 | 90 | 101 | 76 | 121 | 104 | 126 | 39 | nan | nan | nan | . 2011-04 300 | 64 | 61 | 63 | 59 | 68 | 65 | 78 | 22 | nan | nan | nan | nan | . 2011-08 169 | 35 | 42 | 41 | 21 | nan | nan | nan | nan | nan | nan | nan | nan | . 2011-09 299 | 70 | 90 | 34 | nan | nan | nan | nan | nan | nan | nan | nan | nan | . 2011-10 358 | 86 | 41 | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | . 2011-11 324 | 36 | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | . 2011-12 41 | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | . We can see that the first column contains the following . row label (eg. 2010-12) this is the first combination of year and month in which transactions for a given cohort were performed (i.e. this combination defines a unique cohort; in this dataset, there are 10 such cohorts) | . | numerical value (eg. 885) this is the number of customers who made transactions during that combination of year and month (i.e. the number of customers with transactions, for the cohort defined by the row label explained above) | . | . As seen from the raw data, there are 13 months of transaction data so we have 13 columns in the retention matrix. . Visualizing Customer Retention . We&#39;ll start visualizing this data by considering all customers together rather than in cohorts by the month of their first transaction. This are shown below . First_to_Current_InvoiceMonth num_customers pct_users_retained . 0 0 | 4339 | 100.000000 | . 1 1 | 976 | 22.493662 | . 2 2 | 942 | 21.710071 | . 3 3 | 927 | 21.364370 | . 4 4 | 905 | 20.857340 | . 5 5 | 878 | 20.235077 | . 6 6 | 804 | 18.529615 | . 7 7 | 725 | 16.708919 | . 8 8 | 680 | 15.671814 | . 9 9 | 641 | 14.772989 | . 10 10 | 509 | 11.730814 | . 11 11 | 494 | 11.385112 | . 12 12 | 235 | 5.415994 | . We can visualize this as a line chart in a single dimension, rather than as a retention matrix which will capture each cohort on the second dimension . This customer retention curve clearly shows that approximately 75 percent of customers stop making transactions on the online store after the first month. We see a slow but steady decline over the first five months following the initial transaction. Then we see a larger drop after the fourth month to under 19 percent retention, followed by another steady decrease until the end of the eighth month when retention falls to below 12 percent. At the twelveth month after the initial transaction, about 5 percent of users are still active on the online store. . Customers are not quickly getting to the underlying value of the offerings on the web store. Improving the online shopping experience at the store could help get the customer to the core value of the business as quickly as possible, and could boost retention in the process. . We&#39;re also ready to plot the retention matrix data by cohort and inspect the change in number of customers by month of the year. For ease of reading, we will convert these numbers of customers per month, for each cohort, into a percent relative to the first month (column named Month 0) in which transactions were performed by that cohort . cohort_sizes = df_cohort_pivotted[&quot;Month 0&quot;] retention = df_cohort_pivotted.divide(cohort_sizes, axis=0) . The resulting retention matrix is shown below . First and last 5 rows First_to_Current_InvoiceMonth Month 0 Month 1 Month 2 Month 3 Month 4 Month 5 Month 6 Month 7 Month 8 Month 9 Month 10 Month 11 Month 12 . FirstInvoiceDate &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; . 2010-12 1.000 | 0.366 | 0.323 | 0.384 | 0.363 | 0.398 | 0.363 | 0.349 | 0.354 | 0.395 | 0.374 | 0.503 | 0.266 | . 2011-01 1.000 | 0.221 | 0.266 | 0.230 | 0.321 | 0.288 | 0.247 | 0.242 | 0.300 | 0.326 | 0.365 | 0.118 | nan | . 2011-02 1.000 | 0.187 | 0.187 | 0.284 | 0.271 | 0.247 | 0.253 | 0.279 | 0.247 | 0.305 | 0.068 | nan | nan | . 2011-03 1.000 | 0.150 | 0.252 | 0.199 | 0.223 | 0.168 | 0.268 | 0.230 | 0.279 | 0.086 | nan | nan | nan | . 2011-04 1.000 | 0.213 | 0.203 | 0.210 | 0.197 | 0.227 | 0.217 | 0.260 | 0.073 | nan | nan | nan | nan | . 2011-08 1.000 | 0.207 | 0.249 | 0.243 | 0.124 | nan | nan | nan | nan | nan | nan | nan | nan | . 2011-09 1.000 | 0.234 | 0.301 | 0.114 | nan | nan | nan | nan | nan | nan | nan | nan | nan | . 2011-10 1.000 | 0.240 | 0.115 | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | . 2011-11 1.000 | 0.111 | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | . 2011-12 1.000 | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | . We&#39;ll now plot a heatmap of the retention matrix annotated with these fractions expressed as a percentage . The annotation in each cell of this heatmap is the fraction of retained customers (taken as the fraction of the initial size of the cohort that makes monthly transactions after the first month). The stronger the retention the darker the shade of red on this heatmap. . Since the monthly columns were ratioed to the first column (first month of transactions per cohort), they cannot be higher in value (or percentage from 0 percent to 100 percent) than the first column. In terms of the heatmap color scheme used here, this means they cannot appear in a darker shade of red than the first column. At best, they could match the shade of red as the first column but they would need to also be 100 percent. As we can see, there isn&#39;t a month in which the number of unique customers, belonging to a given cohort, making transactions was the same as during the first month. We&#39;ll talk about interpretation of this heatmap next. . By calculating customer retention in these monthly cohorts, we can visualize how long each cohort continues to use the store after their first transaction. . We can see that there isn&#39;t an increasing or decreasing trend of customer behaviour within a cohort. We don&#39;t consistently see more or less customers making transactions in the months since the first transaction. This is true for all cohorts. . For every cohort, the last month for which we have transaction data results in the fewest customers making transactions. This is likely since we don&#39;t have transactions for the entire last month. As we saw from the raw data, the last month for which we have transaction data is December 2011 and we only have nine days of data for this month. . A larger fraction of the first cohort makes transactions after the first month than any other cohort. After the first month, there is no month in which any of the other cohorts outperform the first cohort in the percent of customers making transactions. Even so, there is a nearly a 65 percent drop in customer retention after the first month in this first cohort and between approximately 70 to 90 percent drop in the other cohorts across the subsequent 12 months. . Summary . Cohort analysis can focus on various quantitative metrics. Here, we focused on customer retention at an online retail business. Cohort analysis for customer retention is a powerful tool for developing an understanding of customer behaviour. It facilitates building up an understanding of the drivers for growth, customer engagement and ultimately revenue. Here, it allowed for a direct understanding of which customers leave a business and when they leave. With additional data, it would allow for develpoing an understanding of why customer interaction dropped after the initial month and to make more focused interpretations of the data in terms of their impact on the business. .",
            "url": "https://elsdes3.github.io/analytics-ops-implementation-overload/cohort-analysis-for-customer-retention/exploratory-data-analysis/behavioural-analytics/python/2021/10/24/cohort-analysis.html",
            "relUrl": "/cohort-analysis-for-customer-retention/exploratory-data-analysis/behavioural-analytics/python/2021/10/24/cohort-analysis.html",
            "date": " • Oct 24, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "An End-to-End Implementation of Gradient Descent for Linear Regression",
            "content": "Background . Gradient descent an algorithm for optimization of mathematical functions for which a derivative (or, more generally, gradient) exists at every point for which the function is defined. A derivative (or slope) is the rate of change of the output value of a function relative to its input. In order to optimize such an objective function, we want to minimize its rate of change (derivative). Graphically, we want to start at some point on a plotted curve of the function and move in a direction of decreasing derviative (slope) until it reaches zero (i.e. zero gradient). This is called gradient since we are descending along the curve for the gradient of the function, or going from a high gradient to a low (zero) value. . Goal . Here, we will use Python to implement this algorithm from scratch. Then, we&#39;ll use it to perform multivariate linear regression on synthetic data and compare its performance and attributes to implementations provided by the scikit-learn Python library. . Gradients . For implementing linear regression using gradient descent, we will aim to minimize the mean squared error (MSE) which is the squared difference between the true and predicted values of the dependent variable (y). In calculus, minimizing an objective function requires knowing when the derivative of that function goes to zero. That&#39;s exactly what we&#39;ll be doing here. Since linear regression has both a slope and intercept, we&#39;ll need to calculate the gradient of the function (MSE) with respect to both the slope and intercept separately and we&#39;ll need to find the slope and intercept when these gradients go to zero. The two equations to be minimized are found at these links - 1, 2, 3. . Note that we&#39;ll be considering multi-variable linear regression - with multiple independent variables - so each of these variables will have a coefficient analogous to the slope and we&#39;re looking to leverage gradient descent to find the coefficients (not just a single slope) and bias (or intercept) when the espective gradients go to zero (or are sufficiently close to zero; we&#39;ll have to specify how close is acceptable for our use-case). . The gradient descent algorithm at a high level . Below is the process to be followed to implement a naive version of gradient descent for linear regression . We will start by specifying some initialization of the weights (coefficients) and the bias. For the simple case of linear regression with one independent variable, these would be the slope and the intercept respectively. | Calculate the gradient with respect to the initial guesses for coefficients (weights) and bias. | Next, we will move along the cost function curve in the direction of the steepest descent, or the gradient. This means we want to move along the curve in the negative direction of the gradient. To do this, we will update the current value of the coefficients and bias. For the first step along the curve, these values to be updated will be the initial guesses for coefficient and bias. For the second step, these will be the values that were updated once (during the first step), and similarly for future steps. . Since we want to move in the negative direction (towards the direction of decreasing gradient), we&#39;ll need to subtract some value from each of the initial guesses for coefficients and bias. This will be the update that we perform in order to facilitate moving along the curve. . What we&#39;ll subtract is the product of the learning rate and the gradient . the learning rate will determine how fast we move in the direction of zero (or minimal) slope during each iteration a larger value will reach the minimum faster, but it might go from a non-minimum value to another non-minimum value without crossing the point of zero gradient (this means we might not be descending down the gradient curve as we require) | a smaller value will take longer to reach the minimum, but it won&#39;t jump around (without finding the minimum) as much as a faster learning rate and so is less prone to missing the point of the minimal gradient | . | we will have to choose the learning rate ourselves, so we can compare a few values (large or small) | . | We may or may not want to let this process run indefinitely until we reach the minimum since it may take a prohibitively long time to do so. So, we&#39;ll repeat steps 2. and 3. either until we have reached a minimum or over some pre-determined number of moves (iterations). | Manual Implementation of gradient descent for Linear Regression . We&#39;ll begin the naive implementation of this algorithm, as described above, by creating a custom linear regression class that accepts the three parameters mentioned above - the number of iterations and the learning rate - that we&#39;ll be able to customize later. We&#39;re also going to create attributes for the weights (coefficients) and bias so that they can be accessed by other class methods . class LinearRegressionWithGradientDescent(): def __init__(self, num_iterations=100, learning_rate=0.1): self.num_iterations = num_iterations self.learning_rate = learning_rate self.w = None self.b = None . We&#39;ll now implement the gradient descent algorithm for linear regression. This is done in a .fit() method, in order to follow the API of the scikit-learn package in Python. This method accepts the observations to be used to learn the weights and bias using gradient descent (X) and the dependent variable (y). . We&#39;ll begin by extracting the number of rows and columns in the data as local variables for use later in this method. We&#39;ll initialize the weights and bias to be zeros, although other choices for these initial values are also possible. The weights must be a 1D array of the same length as the number of columns. . We&#39;ll now implement gradient descent to update the weights and bias over a number of iterations. To do this, we&#39;ll calculate the predicted value for the dependent variable by multiplying the weight (initially set to zero) of each column with the values in that column and then add up the results. For single-variable linear regression this is equivalent to taking the product of the slope (m) and the dependent variable (x). Then, we&#39;ll add the bias to the sum we just calculated. Again, in the case of a single-variable linear regression, this is like adding the y-intercept (b) to the product of m and x. Since we might have more than one observation in our data (X), each of these operations is done by multiplying a 1D array of observations with a scalar weight and then added to a scalar bias. . Next, we&#39;ll calculate the gradient. The first part of this is to calculate the error of the prediction. We just calculated the predicted value of the dependent variable and we know its true value (y). The error is the difference between these two. Next, we&#39;ll multiply this error with the corresponding column of true observations. Each 1D array of true observations corresponds to a single column of the data (a 2D array). By multiplying the error with the data, we are scaling each independent variable with the error of its prediction. Since the true observations (y) and the predicted values are 1D arrays, the error is also a 1D array. Finally, we divide the product of the data (scaled by the error of the prediction) by the number of rows in the data (i.e. the number of observations). . To complete the mplementation of gradient descent, we&#39;ll update the intial guesses for the weights and bias. As mentioned above, we&#39;re subtracting the derivative (gradient) from the corresponding initial guesses for the weights and bias. Note that we are over-writing the previous value of the weights and bias so we do not have access to each of these and their corresponding error. This is a limitation of this implementation of gradient descent. . def fit(self, X, y): num_rows, num_cols = X.shape # initial values for weights and bias self.w = np.zeros(num_cols) self.b = 0 # implement gradient descent to update weights and bias for num_iteration in range(self.num_iterations): # predict dependent value using weights and bias y_predicted = np.dot(X, self.w) + self.b # Calculate error of prediction error = y_predicted - y # calculate the gradients gradient_wrt_w = (1 / n_samples) * np.dot(X.T, error) gradient_wrt_b = (1 / n_samples) * np.sum(error) # update parameters self.w -= self.learning_rate * gradient_wrt_w self.b -= self.learning_rate * gradient_wrt_b . We now have a simple implementation of gradient descent and our .fit() method contains the weights and bias for the minimal (best) gradient. We&#39;ll need to use these to predict the dependent variable (y). Inside the .fit() method, we made predictions as part of calculating the error in each iteration but we don&#39;t have access to those predictions since they aren&#39;t class-level attributes. Also, calling a prediction method with the best weights and bias is the approach used by scikit-learn so we&#39;ll also define a new method .predict() to make a prediction using the best weights and bias. . Using the trained algorithm to make a prediction on data it has not seen before is actually very straightforward. For the gradient descent algorithm, this is the same as the prediction we made in the first step of training in .fit(). We&#39;ll take the sum of the product of each column of the unseen data (X_test) and the best weight (which minimized the gradient) of each column (found using X_train) and then add the best bias (also found using X_train) to this sum. . def predict(self, X): y_predicted = np.dot(X, self.w) + self.b return y_predicted . The final linear regression class is shown below . class LinearRegressionWithGradientDescent(BaseEstimator, TransformerMixin): def __init__(self, num_iterations=100, learning_rate=0.1, tol=None): self.num_iterations = num_iterations self.learning_rate = learning_rate self.tol = tol self.w = None self.b = None def fit(self, X, y): num_rows, num_cols = X.shape # initial values for weights and bias self.w = np.zeros(num_cols) self.b = 0 # calculte initial MSE to check for convergence # (if threshold specified) if self.tol: y_predicted = np.dot(X, self.w) + self.b base_mse = mean_squared_error(y_predicted, y) # implement gradient descent to update weights and bias self.cost_func_ = [] for num_iteration in range(self.num_iterations): # predict dependent value using weights and bias y_predicted = np.dot(X, self.w) + self.b # calculate cost or objective function (MSE) mse = mean_squared_error(y_predicted, y) self.cost_func_.append({&quot;iteration&quot;: num_iteration, &quot;cost&quot;: mse}) # calculate the gradients error_term = y_predicted - y gradient_wrt_w = (1 / num_rows) * np.dot(X.T, error_term) gradient_wrt_b = (1 / num_rows) * np.sum(error_term) # move one step along the curve (i.e. update parameters) self.w -= self.learning_rate * gradient_wrt_w self.b -= self.learning_rate * gradient_wrt_b # Stop if algorithm has converged (if thershold is specified) if self.tol: if num_iteration &gt; 0 and np.abs(mse - base_mse) &lt; self.tol: break else: base_mse = mse def predict(self, X): y_predicted = np.dot(X, self.w) + self.b return y_predicted def get_params(self, deep=True): return { &quot;num_iterations&quot;: self.num_iterations, &quot;learning_rate&quot;: self.learning_rate, &quot;tol&quot;: self.tol, } def set_params(self, **parameters): for parameter, value in parameters.items(): setattr(self, parameter, value) return self . Three additions have been made . calculate the MSE in each iteration we&#39;re appending the MSE in each iterateion to an empty list so that we build up a history of the cost function over the iterations. This will allow us to visualize the cost function after training is completed. This history, or evolution of the cost function, is stored in the .cost_func_ attribute of the class. It can only be accessed after training is completed. | . | optionally allow for the option of stopping the algorithm if it has converged.i.e. if the MSE has not decreased, relative to the previous iteration, by atleast some minimum required threshold. The user can specify this threshold. | adding the base classes (BaseEstimator and TransformerMixin) and methods (set_params() and get_params()) to make the class compatible with the scikit-learn | . Using the algorithm with data . So far, we have written Python code to re-create a math equation. We now need to assess how this Python implementation performs when it receives data. By performance, we will restrict ourselves to assessing the MSE (or another regression metric) between . true values of the dependent variable of the data | predictions of the dependent variable, generated from linear regression using our custom gradient-descent-based Python class above | . Assessing the performance of the algorithm . In order to assess how this implementation preforms in practice, we&#39;ll need access to regression data. We&#39;ll use scikit-learn&#39;s make_regression() utility to generate synthetic data with 25 indepdent variables and specify that only 10 of these will hold any power for predicting the dependent variable (or the target) . nrows = 10_000 n_feats = 25 n_informative_feats = 10 noise_factor = 30 . X, y = make_regression( n_samples=nrows, n_features=n_feats, n_informative=n_informative_feats, noise=noise_factor, ) df_X = pd.DataFrame(X, columns=[f&quot;var_{f_i}&quot; for f_i in range(1, 25+1)]) s_y = pd.Series(y, name=&quot;target&quot;) . The first 10 rows of the generated dataset are shown below . First ten rows of independent (var_*) and dependent (target) variables in the data &nbsp; var_1 var_2 var_3 var_4 var_5 var_6 var_7 var_8 var_9 var_10 var_11 var_12 var_13 var_14 var_15 var_16 var_17 var_18 var_19 var_20 var_21 var_22 var_23 var_24 var_25 target . 0 1.792700 | 1.127787 | 1.712581 | 1.011529 | -0.767571 | 1.103263 | 0.290357 | -0.602483 | -0.353389 | -0.398076 | 0.162554 | -1.013285 | -1.048013 | 1.361387 | 0.255620 | -0.219446 | -0.790154 | -0.783198 | -0.192026 | -0.309656 | -0.101219 | -1.673311 | -0.156547 | 0.102527 | -0.420449 | 43.800690 | . 1 0.637377 | 1.700809 | 1.244697 | 0.528072 | -1.895857 | -0.228213 | -1.866266 | 0.041840 | -0.910287 | -0.308629 | -0.040844 | -0.056981 | -0.285775 | -0.458135 | -0.076983 | -0.870040 | 1.621079 | -0.723081 | 2.479969 | 0.986604 | -0.235891 | -0.006206 | 0.431662 | 1.574570 | 0.234945 | -58.335620 | . 2 -0.880964 | 0.690707 | -0.082885 | -0.526609 | -1.727090 | 0.947831 | 0.123300 | 0.758779 | -1.915622 | 0.307955 | -0.102673 | -1.072498 | 1.197150 | -0.875375 | -1.368984 | 0.325383 | 0.423848 | 1.533559 | 0.169389 | 1.228874 | 0.318146 | 0.939685 | 0.667253 | 1.115826 | 0.942457 | 147.990762 | . 3 -0.632239 | 0.372669 | -2.152931 | -1.458230 | 0.491846 | 1.650458 | -1.590618 | -0.847564 | -0.870922 | -0.165997 | 0.211657 | -2.938318 | 0.958410 | 0.171104 | 0.248484 | 2.347177 | -0.267223 | 0.990966 | 0.076983 | 1.456492 | 1.350790 | -0.424085 | 1.580780 | -0.275967 | -0.257916 | 142.439070 | . 4 -0.145482 | -0.643350 | 1.365192 | 1.057258 | -2.248381 | -0.064672 | 0.560886 | 1.269866 | -0.553853 | 0.353916 | 0.965487 | 0.159940 | 0.398602 | -0.708801 | -1.792605 | -0.691393 | -0.400722 | 0.923051 | -1.363650 | -0.286769 | 0.027119 | 1.348968 | -1.765124 | 0.574492 | -0.736973 | -152.714005 | . 5 -1.715326 | -1.475415 | -0.175624 | 0.296775 | -1.580861 | 2.104778 | 0.013798 | -1.219785 | 0.196225 | -0.646695 | -0.866136 | 0.964786 | 0.096211 | -1.190258 | 1.056341 | -0.551378 | -0.586103 | -0.221199 | 0.947428 | 1.725642 | 1.153872 | -0.505651 | 0.117756 | -0.501296 | 0.772890 | 13.270142 | . 6 0.461081 | -0.003744 | -0.381472 | 0.467374 | 0.112163 | -1.348827 | -1.293715 | 0.487404 | -0.111310 | -0.291225 | 2.151299 | -2.168772 | -1.988815 | 1.199534 | 0.703620 | -0.732733 | 1.712295 | -0.301070 | -0.586003 | 0.582624 | -1.671270 | -0.633421 | -0.910792 | 0.135109 | -1.012734 | -340.672472 | . 7 -0.865696 | -0.556232 | -0.477438 | 0.632759 | -0.372946 | -0.048214 | 0.459932 | -0.547575 | -0.425400 | 1.685614 | 0.847741 | 0.544747 | 2.075885 | -1.034107 | -0.150349 | -1.835626 | 0.715219 | 1.303508 | -1.667222 | -0.224243 | -0.945824 | -0.807895 | -1.536962 | 0.247709 | -0.197013 | -48.009676 | . 8 -0.547513 | -0.663081 | -1.297435 | -0.413594 | -0.392603 | 0.171738 | -0.715886 | 0.317824 | -0.142584 | 0.578596 | 0.692859 | 0.763659 | -1.208714 | -0.069916 | 0.140127 | -0.518910 | 0.839801 | -1.017963 | 1.150335 | 0.197999 | 2.161302 | -3.318029 | 0.402641 | -0.381060 | 0.131789 | -141.473663 | . 9 -0.380503 | -0.661251 | 0.514581 | -1.084259 | -0.178863 | 0.886637 | 1.163898 | 0.950750 | -1.063634 | -1.595980 | -0.128710 | 1.525115 | 0.046002 | -1.866864 | 0.021437 | 1.267028 | 0.239687 | -0.052232 | 0.234784 | -1.269702 | -0.040290 | 0.855980 | -1.434854 | 0.901828 | 1.440784 | 274.600917 | . Dividing the data for use in training and evaluation . We&#39;ll divide the data into a training and testing split. We&#39;ll use the training split to learn the best weights (coefficients) and bias and then assess their performance on the data that was not seen during the gradient descent process. . We&#39;ll use scikit-learn&#39;s train_test_split() to create these divisions of the overall dataset, with a random 30 percent set aside of evaluation and the remainder to be used during training . X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30) . Preprocessing the data . We&#39;ll show a heatmap of the correlation between all pairs of independent variables . As we can see, the correlation values cover a small range around zero so there isn&#39;t any evidence of multi-collinearity and we don&#39;t need select independent variables in order to interpret the best-fit coefficients. Note that this is specific to the dataset we are using here and is very unlikely in a real dataset. . Below, we&#39;ll show the mean, standard deviation and variance of each of the independent variables in the training data . d_ranges = {} d_avgs = { f&quot;{f}__{split_type}__mean&quot;: val for X_split, split_type in zip([X_train, X_test], [&quot;train&quot;]) for val,f in zip(np.mean(X_split, axis=0).tolist(), list(df_X)[:-1]) } d_stds = { f&quot;{f}__{split_type}__std&quot;: val for X_split, split_type in zip([X_train, X_test], [&quot;train&quot;]) for val,f in zip(np.std(X_split, axis=0).tolist(), list(df_X)[:-1]) } d_vars = { f&quot;{f}__{split_type}__var&quot;: val for X_split, split_type in zip([X_train, X_test], [&quot;train&quot;]) for val,f in zip(np.var(X_split, axis=0).tolist(), list(df_X)[:-1]) } d_ranges.update(d_avgs) d_ranges.update(d_stds) d_ranges.update(d_vars) df_ranges = pd.DataFrame.from_dict( d_ranges, orient=&quot;index&quot;, ).rename(columns={0: &quot;Value&quot;}).reset_index() df_ranges[[&quot;Column&quot;, &quot;Split&quot;, &quot;Statistic&quot;]] = df_ranges[&quot;index&quot;].str.split(&quot;__&quot;, expand=True) df_ranges = df_ranges[[&quot;Column&quot;, &quot;Split&quot;, &quot;Statistic&quot;, &quot;Value&quot;]] df_ranges_pivoted = df_ranges.pivot(index=&quot;Column&quot;, columns=[&quot;Statistic&quot;], values=&quot;Value&quot;) display(df_ranges_pivoted.style.format(&quot;{:,.5f}&quot;).set_caption(&quot;Stats for Training Data&quot;)) . Stats for Training Data Statistic mean std var . Column &nbsp; &nbsp; &nbsp; . var_1 0.01168 | 0.98933 | 0.97877 | . var_10 0.00622 | 1.00519 | 1.01041 | . var_11 0.00403 | 0.99770 | 0.99541 | . var_12 -0.02262 | 0.99986 | 0.99972 | . var_13 0.02241 | 1.01269 | 1.02554 | . var_14 -0.01230 | 0.99987 | 0.99974 | . var_15 -0.01553 | 1.00688 | 1.01381 | . var_16 -0.01859 | 1.00160 | 1.00319 | . var_17 -0.00405 | 0.99642 | 0.99285 | . var_18 0.01692 | 1.00669 | 1.01343 | . var_19 -0.00170 | 0.98580 | 0.97180 | . var_2 0.00643 | 1.00400 | 1.00801 | . var_20 -0.00033 | 1.00143 | 1.00285 | . var_21 -0.00267 | 1.00811 | 1.01628 | . var_22 0.00908 | 0.99283 | 0.98571 | . var_23 0.00645 | 0.99164 | 0.98335 | . var_24 0.01746 | 0.99420 | 0.98844 | . var_3 0.01159 | 1.00577 | 1.01157 | . var_4 0.00066 | 0.99118 | 0.98244 | . var_5 -0.01073 | 1.00872 | 1.01752 | . var_6 0.00182 | 1.00632 | 1.01268 | . var_7 0.01633 | 1.01769 | 1.03570 | . var_8 -0.00026 | 0.99083 | 0.98175 | . var_9 -0.00083 | 1.02609 | 1.05286 | . We can see that each of the independent variables in the training data has a mean of zero and a standard deviation of close to 1. This is a consequence of the dataset that we have created here. In more realistic use-cases, this is not the case and so we would have to, for example, standardize the data (independent variables) before performing linear regression. Some machine learning models perform better with such pre-processed data than on raw data and some others required it. This pre-processing is particularly helpful when trying to interpret the weights, after training is completed, since this isn&#39;t possible if each of the independent variables covers a different range from the others. . We can make use of scikit-learn&#39;s Pipeline class to chain together the standardization and linear regression steps in order to optimize the weights and bias. Once this training step is completed, the regression step of the pipeline will be set to the best coefficients and bias. Recall that these were defined in the __init__ method of the custom Python class and then updated during training in .fit(). After training is completed, their values in the class are those at the end of the gradient descent process. Calling .predict() after .fit() ensures that we make a prediction using the best-fit values found during training. After .fit() has completed, any subsequent call to .predict() will make a prediction using the best-fit values found during training. . In this way, after training, we will use this pipeline with best-fit values for coefficients and bias to make a prediction on the test data . pipe_manual = Pipeline( [ (&quot;ss&quot;, StandardScaler()), (&quot;reg&quot;, LinearRegressionWithGradientDescent(100, 0.05, None)), ] ) pipe_manual.fit(X_train, y_train) y_pred_manual = pipe_manual.predict(X_test) . Checking that the algorithm is working correctly - Examine the cost function . We are keeping track of the objective (or cost) function (MSE) at each iteration. After training is completed, we can access the cost function evoluton by iteration, or training history. Below, we&#39;ll extract the cost function for each iteration . df_cost = pd.DataFrame.from_records(pipe_manual.named_steps[&quot;reg&quot;].cost_func_) . The first 10 rows of the cost function evolution by iteration are shown below . First 10 rows of Cost Function &nbsp; iteration cost . 0 0 | 24307.979855 | . 1 1 | 22031.332903 | . 2 2 | 19976.771709 | . 3 3 | 18122.554856 | . 4 4 | 16449.076952 | . 5 5 | 14938.658014 | . 6 6 | 13575.353707 | . 7 7 | 12344.784339 | . 8 8 | 11233.980775 | . 9 9 | 10231.245573 | . Plot the full cost-function training history with and without a tolerance threshold . pipe_manual_tol = Pipeline( [ (&quot;ss&quot;, StandardScaler()), (&quot;reg&quot;, LinearRegressionWithGradientDescent(100, 0.05, 30)), ] ) pipe_manual_tol.fit(X_train, y_train) df_cost_tol = pd.DataFrame.from_records(pipe_manual_tol.named_steps[&quot;reg&quot;].cost_func_) . The cost function is decreasing as the number of iterations is increased. This is the behaviour we expected so it is reassuring to see this exhibited from our manual implementation of the algorithm. . If gradient descent is performing efficiently, then the cost function should be decreasing after each iteration. We can clearly see that, without the tolerance threshold, we have specified too many iterations. We only needed approximately 60 iterations to minimize the gradient. If the gradient descent algorithm can no longer decrease, the cost function will be constant and is said to have converged. We can see this convergence starting after approximately 60 iterations for our synthetic dataset and for the choice of learning rate (0.05) and without a MSE stopping threshold. The stopping criteria threshold we specified cutoff the algorithm after approximately 45 iterations. So, for this learning rate, we should decrease the number of iterations (without a stopping tolerance) or adjust the stopping tolerance to capture approximately 15 more iterations. . If the cost function is increasing with every iteration, then this might point to a poor choice of initial learning rate. As mentioned earlier, if the learning rate is too high then the algorithm might never reach a minimum and it could just be jumping along the curve in the direction of maximizing the gradient which is not the intended behaviour. The curve won&#39;t show convergence of the cost function. Luckily, this isn&#39;t the case here so we have atleast picked a reasonable starting value for the learning rate. . Displaying the predicted values and Best-fit Weights (Coefficients) and Bias . Our custom linear regression model&#39;s first 10 predictions of the test split are shown below, compared to the corresponding true values . display( pd.DataFrame(X_test, columns=list(df_X)).head(10).merge( pd.Series(y_test, name=&quot;target&quot;).head(10).to_frame().merge( pd.Series(y_pred_manual,name=&quot;predicted_target&quot;,).head(10).to_frame(), left_index=True, right_index=True, ), left_index=True, right_index=True, ).style.set_caption(&quot;First 10 observations and known and preducted target values of test split&quot;) ) . First 10 observations and known and preducted target values of test split &nbsp; var_1 var_2 var_3 var_4 var_5 var_6 var_7 var_8 var_9 var_10 var_11 var_12 var_13 var_14 var_15 var_16 var_17 var_18 var_19 var_20 var_21 var_22 var_23 var_24 var_25 target predicted_target . 0 -1.923454 | -1.796301 | -0.982786 | 0.457604 | -0.104491 | 1.795717 | -0.186553 | -0.880441 | -0.610919 | 0.985903 | 0.119960 | 0.532591 | -0.872828 | 0.624335 | 1.051898 | -0.203395 | 1.235361 | 0.028513 | -0.073853 | -1.742632 | 0.233739 | -0.926359 | -1.407668 | 0.992290 | -0.694696 | 212.514966 | 173.344885 | . 1 -1.056531 | -0.747208 | 0.454180 | 0.924427 | 1.805070 | -0.110764 | -0.163229 | 1.110975 | 0.029911 | -0.764750 | 0.690778 | 1.526860 | -1.187208 | 0.102705 | 0.827845 | -1.044570 | -1.173953 | 0.810510 | -1.910594 | -0.033122 | 0.540827 | 0.001170 | -1.264303 | 0.768591 | -0.293051 | 52.367652 | 36.894715 | . 2 -1.042418 | -0.821429 | -0.062885 | -0.148327 | -1.000657 | 0.516871 | 0.076788 | -0.224269 | -0.117229 | 0.004950 | 0.629484 | -0.868521 | -0.043359 | 0.109846 | -0.027850 | -2.306467 | 1.833603 | -2.058016 | -0.711670 | -0.380086 | 0.202176 | -0.429341 | 0.523285 | -0.030454 | -0.526949 | -163.179868 | -195.917084 | . 3 -0.046920 | 1.118856 | 1.625911 | -0.511468 | 0.207604 | 0.361096 | -2.121240 | 0.077040 | -0.344934 | 0.010806 | 0.682866 | 1.131704 | 1.917818 | 1.344261 | 0.888553 | 0.375325 | 0.810015 | -0.376201 | -0.538603 | 1.010214 | -0.969557 | 0.591528 | 0.583891 | -0.888560 | 0.349702 | 97.391584 | 70.191637 | . 4 0.047814 | 0.759223 | -0.084645 | -0.771253 | 0.460726 | 1.139622 | 0.185418 | 0.715470 | 0.344126 | -0.293447 | 0.442763 | 0.089221 | -0.923057 | -1.508122 | -0.145563 | 1.421391 | 1.137941 | 0.005176 | 2.545589 | -0.442619 | 0.970858 | -1.854820 | 1.103134 | 0.186033 | 1.350449 | 249.593456 | 207.121306 | . 5 2.139370 | 0.961131 | -1.544328 | -0.512599 | -0.069124 | -1.335064 | -1.098256 | 0.341811 | -0.390806 | -1.100419 | -1.465510 | 1.367418 | -1.404622 | -0.025031 | 0.389392 | 0.352283 | 0.567158 | -0.429782 | -0.658122 | -0.698537 | 0.040165 | -1.985189 | -0.525582 | 0.487182 | -0.917951 | -73.027802 | -87.834292 | . 6 1.655406 | 1.283448 | -0.241465 | 0.503443 | -0.297215 | -0.315656 | 1.855960 | -0.625804 | 0.945834 | -1.196963 | 0.998408 | -0.087789 | -0.328202 | -0.264955 | -0.077038 | 1.217371 | 1.645096 | 0.316373 | -1.244701 | 0.105713 | 1.606789 | 0.721503 | 0.842319 | 0.225852 | 0.873367 | 106.233229 | 112.477617 | . 7 -0.907854 | 0.295983 | 0.607493 | 1.665750 | 0.511770 | -0.936718 | -0.395667 | 1.304642 | -0.413647 | 1.597039 | 0.378173 | 0.716501 | 0.366916 | -2.083366 | 1.314320 | -1.469228 | 1.460093 | -1.493714 | -2.354702 | 0.116064 | 0.823188 | 0.392936 | 0.611410 | 1.485290 | 0.053949 | -80.514153 | -67.583225 | . 8 0.962547 | 1.189217 | 0.122271 | -0.255375 | -2.325202 | -0.134592 | 0.828324 | -0.705864 | 0.427568 | 2.807401 | 0.202110 | 1.595346 | -0.535842 | 0.170101 | 0.189906 | 1.543387 | 0.807549 | -1.120124 | 0.354840 | -0.507901 | -1.177085 | -1.986059 | 0.584541 | -0.459629 | -0.234898 | 72.670844 | 39.427636 | . 9 0.090098 | -0.697002 | -0.900997 | -1.232217 | 0.299003 | -0.068079 | 0.072387 | -1.700154 | -0.001606 | -1.233060 | 0.641564 | 0.964712 | -0.983009 | 0.747980 | -0.257779 | -0.402535 | -1.578612 | 0.582391 | 1.595396 | 0.979283 | -0.408286 | -0.591740 | -0.018303 | 1.429989 | 2.591537 | 173.832368 | 96.837718 | . Compare Scoring Performance to scikit-learn&#39;s implementations . We can similarly make predictions using the LinearRegression() module offered by scikit-learn, which is based on the least squares minimization approach . pipe_builtin = Pipeline( [ (&quot;ss&quot;, StandardScaler()), (&quot;reg&quot;, LinearRegression()), ] ) pipe_builtin.fit(X_train, y_train) y_pred_builtin = pipe_builtin.predict(X_test) . We can repeat this process with the scikit-learn SGDRegressor() module (using the same number of iterations as was used in our manual implementation here), which uses a variant of gradient descent called stochastic gradient descent explained here . pipe_builtin_sgd = Pipeline( [ (&quot;ss&quot;, StandardScaler()), (&quot;reg&quot;, SGDRegressor(max_iter=100)), ] ) pipe_builtin_sgd.fit(X_train, y_train) y_pred_builtin_sgd = pipe_builtin_sgd.predict(X_test) . First, we can compare the best weights (coefficients) determined by our custom linear regression model after gradient descent has completed and the two built-in versions . df_coefs = ( pd.Series( pipe_manual.named_steps[&quot;reg&quot;].w, name=&quot;best_coefficient&quot;, index=list(df_X) ).to_frame().assign(best_bias=pipe_manual.named_steps[&quot;reg&quot;].b).sort_values( by=[&quot;best_coefficient&quot;], ascending=False ).reset_index().rename(columns={&quot;index&quot;: &quot;independent_variable&quot;}) ) df_coefs_builtin_lr = ( pd.Series( pipe_builtin.named_steps[&quot;reg&quot;].coef_, name=&quot;best_coefficient_lr&quot;, index=list(df_X) ).to_frame().assign( best_bias_lr=pipe_builtin.named_steps[&quot;reg&quot;].intercept_ ).sort_values( by=[&quot;best_coefficient_lr&quot;], ascending=False ).reset_index().rename(columns={&quot;index&quot;: &quot;independent_variable&quot;}) ) df_coefs_builtin_sgd = ( pd.Series( pipe_builtin_sgd.named_steps[&quot;reg&quot;].coef_, name=&quot;best_coefficient_sgd&quot;, index=list(df_X) ).to_frame().assign( best_bias_sgd=pipe_builtin_sgd.named_steps[&quot;reg&quot;].intercept_[0] ).sort_values( by=[&quot;best_coefficient_sgd&quot;], ascending=False ).reset_index().rename(columns={&quot;index&quot;: &quot;independent_variable&quot;}) ) coef_cols = [&quot;best_coefficient&quot;, &quot;best_coefficient_lr&quot;, &quot;best_coefficient_sgd&quot;] replace_dict = { &quot;best_coefficient_sgd&quot;: &quot;sgd&quot;, &quot;best_coefficient_lr&quot;: &quot;lr&quot;, &quot;best_coefficient&quot;: &quot;manual&quot;, } df_coefs_all = df_coefs.merge( df_coefs_builtin_lr, on=&quot;independent_variable&quot; ).merge( df_coefs_builtin_sgd, on=&quot;independent_variable&quot;, ).set_index(&quot;independent_variable&quot;) df_coefs_all_reshaped = df_coefs_all[coef_cols].unstack().reset_index().rename( columns={&quot;level_0&quot;: &quot;model&quot;, 0: &quot;value&quot;} ) df_coefs_all_reshaped[&quot;model&quot;] = df_coefs_all_reshaped[&quot;model&quot;].map(replace_dict) . . For all three linear regression models, we can see that nine of these independent variables have an order of magnitude larger coefficient than the others. This is plausible since, when generating the synthetic dataset, we indicated that 10 independent variables should be useful for predicting the target variable. . Generally the coefficients from the built-in implementation of linear regression using gradient descent, with the specific choice of learning rate used here (0.05), shows good agreement with the two built-in versions. Recall that the same number of iterations was specified for the manual implementation and built-in version of stochastic gradient descent. . As mentioned earlier, this custom implementation does not track the individual weights (coefficients) and bias per iteration. So, we cannot access these after they are learnt using the training data but we can access the best coefficients and bias that are updated after the final iteration is completed. . Next, we show a comparison of two scoring metrics on predictions of data not seen during the process of determining the weights and bias (i.e. during training) using our custom implementation of the linear regression class (relying on gradient descent) compared to the scikit-learn&#39;s implementations explored above . Performance Comparison &nbsp; metric algorithm OOS Score . 0 R^2 | manual | 0.962056 | . 1 R^2-LR | built-in | 0.962041 | . 2 R^2-SGD | built-in | 0.961891 | . 3 MSE | manual | 909.425659 | . 4 MSE-LR | built-in | 909.793066 | . 5 MSE-SGD | built-in | 913.381884 | . As we can see, the results are quite encouraging as the naive approach compares favourably to the versions of linear regression models offered by scikit-learn. Ofcourse, efficiency will likely be truly challenged on larger and more realistic datasets, but that is beyond the current scope. . HyperParameter Tuning for the Custom Linear Regression Implementation . It is also possible to tune hyper-parameters - learning rate, number of iterations and tolerance - that our custom implementation exposes to the user. We can do this, for example, by defining a grid of values, all combinations of which will be injected into the class before training and making predictions on held-out data. A simple grid across learning rate and number of iterations is shown below . params_grid = { &quot;reg__num_iterations&quot;: [20, 50, 100, 500, 1_000, 2_000], &quot;reg__learning_rate&quot;: [0.05, 0.01, 0.005, 0.001], } . Rather than scoring predictions on a single test split, we can use cross-validation to create multiple divisions of the training data, each with its own training and hold-out split, and score the predictions on each such hold-out split. Since the custom linear regressor class follows the scikit-learn API, we can make use GridSearchCV() to perform this tuning. We&#39;ll use 5-fold cross-validation, choose MSE as the scoring metric and use multi-core computing to have each instance of the custom linear regression class scored in parallel as shown below . %%time gs = GridSearchCV( pipe_manual, param_grid=params_grid, cv=5, scoring=&quot;neg_mean_squared_error&quot;, return_train_score=True, n_jobs=-1, ) _ = gs.fit(X_train, y_train) . CPU times: user 22.2 s, sys: 1min 27s, total: 1min 49s Wall time: 29.5 s . The scores of the predictions on each of the training and hold-out (or validation) sub-splits of the overall training data are shown below . Summary of HyperParameter Tuning with Grid Search &nbsp; param_reg__learning_rate param_reg__num_iterations mean_test_score std_test_score mean_train_score std_train_score . 17 0.005000 | 2000 | 894.74 | 48.03 | 886.59 | 12.08 | . 10 0.010000 | 1000 | 894.74 | 48.03 | 886.59 | 12.08 | . 11 0.010000 | 2000 | 894.74 | 48.04 | 886.59 | 12.08 | . 3 0.050000 | 500 | 894.74 | 48.04 | 886.59 | 12.08 | . 4 0.050000 | 1000 | 894.74 | 48.04 | 886.59 | 12.08 | . 5 0.050000 | 2000 | 894.74 | 48.04 | 886.59 | 12.08 | . 2 0.050000 | 100 | 895.55 | 47.48 | 887.64 | 12.09 | . 9 0.010000 | 500 | 895.76 | 47.45 | 887.87 | 12.09 | . 16 0.005000 | 1000 | 895.79 | 47.44 | 887.90 | 12.09 | . 1 0.050000 | 50 | 1,044.48 | 47.50 | 1,035.43 | 12.18 | . 15 0.005000 | 500 | 1,062.21 | 47.83 | 1,053.02 | 12.17 | . 23 0.001000 | 2000 | 1,345.89 | 52.06 | 1,333.95 | 12.73 | . 0 0.050000 | 20 | 3,967.38 | 90.45 | 3,940.20 | 18.27 | . 8 0.010000 | 100 | 4,094.83 | 92.23 | 4,067.68 | 17.91 | . 22 0.001000 | 1000 | 4,123.27 | 92.62 | 4,096.13 | 17.82 | . 7 0.010000 | 50 | 9,533.50 | 171.50 | 9,499.08 | 13.31 | . 14 0.005000 | 100 | 9,554.99 | 171.75 | 9,520.66 | 13.29 | . 21 0.001000 | 500 | 9,572.12 | 171.95 | 9,537.85 | 13.26 | . 13 0.005000 | 50 | 15,126.98 | 250.47 | 15,099.15 | 28.88 | . 6 0.010000 | 20 | 16,599.84 | 270.75 | 16,575.20 | 37.20 | . 12 0.005000 | 20 | 20,078.41 | 317.52 | 20,063.48 | 60.42 | . 20 0.001000 | 100 | 20,086.04 | 317.59 | 20,071.17 | 60.51 | . 19 0.001000 | 50 | 22,091.71 | 344.01 | 22,083.51 | 75.49 | . 18 0.001000 | 20 | 23,394.83 | 360.95 | 23,391.35 | 85.75 | . As we can see, for this particular dataset and using the choices of hyper-parameter values specified above for our custom gradient-descent-based linear regression class, higher learning rates (0.01 to 0.05) with a wide range of number of iterations, or low learning rates with 1,000 or more iterations, score better in terms of minimizing the average MSE across all the training and validation folds. There is minor evidence of over-fitting, but it is not particularly strong. . Resources &amp; Further Reading . The scoring performance of this naive implementation of gradient descent is promising, though this was only explored on a small synthetic dataset. The adjustable functionality that is exposed via this custom implementation is restricted to the number of iterations and the learning rate. . The naive version of gradient descent used here is a version of the so-called mini-batch gradient descent. In another version, it is also possible to pass sections of the data to the algorithm. This version is called stochastic gradient descent as mentioned earlier, which takes samples of the dataset in each iteration. This is the principle of the second built-in version (SGDRegressor() class provided by scikit-learn) used here. There is more sophisticated functionality offered by this built-in version. For example . the learning rate can be specified as a strategy rather than as a simple floating point number as was the case here | it is also possible to use early-stopping to stop the algorithm running when the MSE on a held-out split of the training data stops improving by atleast some minimum threshold that we can specify in our manual implementation, we used all the training data to calculate the MSE for early stopping | . | we can stop if no improvement is seen in the validation score after a certain number of iterations | the dataset used during training can also be shuffled between iterations | . See the documentation for more details about using gradient descent for linear regression with this class. . Each flavour of the algorithm has its own advantages and distavantages. Read more about Stochastic gradient descent here (1, 2) to see how it compares to the batch-based approach. .",
            "url": "https://elsdes3.github.io/analytics-ops-implementation-overload/gradient-descent/linear-regression/ml-algorithm-from-scratch/machine-learning/2021/10/23/lr-grad-descent.html",
            "relUrl": "/gradient-descent/linear-regression/ml-algorithm-from-scratch/machine-learning/2021/10/23/lr-grad-descent.html",
            "date": " • Oct 23, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I’m a data consultant and Python developer. I use data and technology to help solve important problems. I’m passionate about the operational aspects of data analytics and write about these and related topics in my blogs here. .",
          "url": "https://elsdes3.github.io/analytics-ops-implementation-overload/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://elsdes3.github.io/analytics-ops-implementation-overload/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}